{"cells":[{"cell_type":"markdown","metadata":{"id":"C192SOmJS6lw"},"source":["# CS 195: Natural Language Processing\n","## Attention and Transformers\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F7_2_AttentionTransformers.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"a0R2z0do2M6v"},"source":["## Reference\n","\n","SLP: Attention, Section 9.8 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/9.pdf\n","\n","SLP: Transformers and Pre-Trained Language Models, Chapter 10 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/10.pdf\n","\n","How do Transformers Work?, Section 1.4 of Hugging Face NLP Course https://huggingface.co/learn/nlp-course/chapter1/4"]},{"cell_type":"markdown","metadata":{"id":"BuYZq_te2M6x"},"source":["## Reminder: Applied Exploration\n","\n","The applied exploration for this fortnight will be a little different. I want everyone to get some experience fine-tuning an existing model, so this will be the task for the entire fortnight.\n","\n","See the [workshop from last time](https://github.com/ericmanley/F23-CS195NLP/blob/main/F7_1_TransferLearning.ipynb)\n","\n","Fine-tune an existing model with the following requirements\n","* Choose a different starting model - you can use any Hugging Face model, but consider starting with a general one like BART or Llama2.\n","* Choose a different data set - think about something that would be good to include in an application that interests you\n","* Evaluate how well it performed. For sequence-to-sequence model, try going back and using Rouge from Fortnight 1.\n","\n","The Hugging Face NLP course has [examples of fine-tuning for many different tasks](https://huggingface.co/learn/nlp-course/chapter7/1)."]},{"cell_type":"markdown","metadata":{"id":"PtCQtwFn2M6y"},"source":["## Preliminary Discussion\n","\n","We often represent things (word embeddings, context, hidden state, etc.) in neural networks as vectors.\n","* interpreted mathematically, vectors have *direction* and *magnitude*\n","\n","Sometimes we want to be able to tell how *similar* two vectors are - for example, two similar words might have similar embedding vectors\n","* similar vectors point in the same direction\n","* we usually *normalize* them so that they're of length 1 but keep the same direction\n","\n","Which of the following two sets of vectors are more similar?\n","\n","(0.383, 0.077, 0.920) and (0.477, 0.191, 0.858)\n","\n","or\n","\n","(0.383, 0.077, 0.920) and  (0.759, 0.569, 0.316)"]},{"cell_type":"markdown","metadata":{"id":"O2va710a2M6z"},"source":["## Dot products\n","\n","A **dot product** of two vectors is one way to measure similarity\n","\n","Compute by\n","* multiplying corresponding entries\n","* adding them all together"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"uq8dCgw52M60","outputId":"019bbaa0-4c70-4944-9a4f-f2a4db047b2c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701207282012,"user_tz":360,"elapsed":7,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.986758"]},"metadata":{},"execution_count":1}],"source":["(0.383*0.477) + (0.077*0.191) + (0.920*0.858)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"sRVrGiAj2M63","outputId":"ba3ed03a-1c2b-4cd6-8cd3-d0100a5169e6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701207282012,"user_tz":360,"elapsed":4,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.62523"]},"metadata":{},"execution_count":2}],"source":["(0.383*0.759) + (0.077*0.569) + (0.920*0.316)"]},{"cell_type":"markdown","metadata":{"id":"VLzjBg3i2M64"},"source":["## Review: The Encoder-Decoder Recurrent Neural Network Architecture\n","\n","<div>\n","    <center>\n","    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/encoder-decoder_detail.png?raw=1\">\n","    </center>\n","</div>\n","\n","### Discuss:\n","1. What do encoders do? How are they trained?\n","2. What do decoders do? How are they trained.\n","3. What is **c** in this diagram and what is its purpose?\n","\n","\n","image source: SLP Fig. 9.18, https://web.stanford.edu/~jurafsky/slp3/9.pdf"]},{"cell_type":"markdown","source":["Notes:\n","*   Encoders trained using masked language tasks\n","*   Ignore the output of the encoder and take its training weights, that's the valuable part of the encoder\n","* Decoders output the desired result based on passed vector\n","* Decoders typically trained using \"predict the next word\" tasks\n","* C is the \"essence\" vector\n","\n"],"metadata":{"id":"YzIc4KAC6SOV"}},{"cell_type":"markdown","metadata":{"id":"TfNmZ2o02M64"},"source":["## Problem: Bottleneck\n","\n","<div>\n","    <center>\n","    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/bottleneck.png?raw=1\" width=700>\n","    </center>\n","</div>\n","\n","The final hidden state in the encoder has to contain everything meaningful about the input text\n","* may not represent things from earlier in the input sequence\n","* even if you use LSTM nodes\n","\n","image source: SLP Fig. 9.21, https://web.stanford.edu/~jurafsky/slp3/9.pdf"]},{"cell_type":"markdown","metadata":{"id":"78jtnPoy2M65"},"source":["## Attention\n","\n","\n","\n","<div>\n","    <center>\n","    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/attention.png?raw=1\" width=800>\n","    </center>\n","</div>\n","\n","**Idea:** instead of just passing the final hidden encoder state to the decoder, pass a combination of all encoder states\n","* weighted sum: makes sure that the context vector is a fixed size (can be some other more complicated function)\n","* computed again for each decoder state $i$\n","* takes into account decoder state $i-1$ and all encoder states\n","* you can learn *which input words are most important* when generating the next word\n","* even better at retaining long-term information\n","\n","image source: SLP Fig. 9.23, https://web.stanford.edu/~jurafsky/slp3/9.pdf"]},{"cell_type":"markdown","metadata":{"id":"5EnzCrRW2M65"},"source":["## Computing the Attention context vector\n","\n","1. compute the *score* for how relevant each encoder state is to each decoder state\n","    * can be simple - the dot product\n","    * this is a single number that represents how similar the two vectors are\n","    * tells us the relevance of each encoder state to the current step in the decoder\n","2. Normalize these with a softmax to create a vector of weights $\\alpha_{ij}$ - this essentially turns the relevance for each encoder state into probabilities\n","3. Use these normalized relevence scores as weights in a weighted sum - this is our new context vector $$c_i = \\sum_{j} \\alpha_{ij}h^e_j$$"]},{"cell_type":"markdown","metadata":{"id":"vfHJHUy62M66"},"source":["## Attention is all you need\n","\n","2017: Big breakthrough by researchers at Google - the **Transformer**\n","* You can use attention *without recurrent structures*\n","* recurrent structures: slow training - you have to generate them sequentially\n","* transformers: fast training - you can do it in parallel\n","\n","<div>\n","    <center>\n","    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/attention_paper.png?raw=1\">\n","    </center>\n","</div>\n","\n","Paper available at https://arxiv.org/abs/1706.03762"]},{"cell_type":"markdown","metadata":{"id":"-HcEWFDg2M66"},"source":["## Transformers\n","\n","Transformers include\n","* linear layers\n","* feedforward networks\n","* **self-attention** layers\n","    - directly extract and use information from arbitrarily large contexts\n","    \n"]},{"cell_type":"markdown","metadata":{"id":"1qU30JVN2M67"},"source":["## A simple self attention layer\n","\n","<div>\n","    <center>\n","    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/simple_self_attention.png?raw=1\">\n","    </center>\n","</div>\n","\n","* Each output is based on all prior inputs\n","\n","* Similar to recurrent calculation\n","    - compute similarity score of each pair $x_i$ and $x_j$ (can be just dot product)\n","    - normalize with softmax, call it $\\alpha_{ij}$\n","    - generate output as weighted sum of inputs $$y_i = \\sum_{j \\leq i} \\alpha_{ij}x_j$$\n","    \n","* **Note that these can be computed in parallel!**\n","\n","image source: SLP Fig. 10.1, https://web.stanford.edu/~jurafsky/slp3/10.pdf"]},{"cell_type":"markdown","metadata":{"id":"hU7cEtMO2M67"},"source":["## Fancier Embeddings\n","\n","Transformers generate three new vectors from each word embedding representing different roles a word can play\n","\n","\n","<div>\n","    <center>\n","    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/query_key_value.png?raw=1\" width=600>\n","    </center>\n","</div>\n","\n","* **Query vector:** used to measure relevance a word should give to other words in a sentence\n","    - current focus of attention\n","* **Key vector:** the vector that a query vector is compared to - how much focus the query word should give to this word\n","    - preceding words being compared to the current focus\n","* **Value vector:** information from the word that should be passed to the other words\n","    - once query and key have been matched, output is mostly the value vector but guided by interaction of query and key\n","    \n","*All of these weights are learned as part of the training process!*\n","\n","image source: SLP Fig. 10.2, https://web.stanford.edu/~jurafsky/slp3/10.pdf"]},{"cell_type":"markdown","metadata":{"id":"BAQmrTS92M67"},"source":["## Transformer Block\n","\n","<div>\n","    <center>\n","    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/transformer_block.png?raw=1\">\n","    </center>\n","</div>\n","\n","**Residual connections:** Allow information to skip layers - improves learning, helps with the vanishing gradient\n","\n","**Normalization layers:** Rescale the vectors so that they're all meastured on the same scale (like z-score normalization in statistics) - also includes some learnable parameters called gain and offset for multiplying or adding on to the scaled values\n","\n","image source: SLP Fig. 10.4, https://web.stanford.edu/~jurafsky/slp3/10.pdf"]},{"cell_type":"markdown","metadata":{"id":"mCvt9pP72M67"},"source":["## Multi-Headed Attention\n","\n","<div>\n","    <center>\n","    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/multiheaded_attention.png?raw=1\">\n","    </center>\n","</div>\n","\n","As we have seen, the same word can have many different senses (e.g., a river *bank* vs. a *bank* that is a financial institution)\n","\n","This can happen at each level of abstraction in a neural network language model\n","\n","Each **head** is a self-attention layer capable of handling different relationships between words\n","\n","image source: SLP Fig. 10.5, https://web.stanford.edu/~jurafsky/slp3/10.pdf"]},{"cell_type":"markdown","metadata":{"id":"koGvJUwI2M68"},"source":["## Diagram of Encoder-Decoder Transformer\n","\n","Encoder is on left, decoder on right\n","\n","<div>\n","    <center>\n","    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/transformer_encoder_decoder.svg?raw=1\">\n","    </center>\n","</div>\n","\n","\n","image source: https://huggingface.co/learn/nlp-course/chapter1/4"]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[{"file_id":"https://github.com/ericmanley/f23-CS195NLP/blob/main/F7_2_AttentionTransformers.ipynb","timestamp":1701207136714}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}