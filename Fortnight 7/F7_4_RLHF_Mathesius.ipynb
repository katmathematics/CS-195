{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C192SOmJS6lw"
      },
      "source": [
        "# CS 195: Natural Language Processing\n",
        "## Reinforcement Learning from Human Feedback\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F7_4_RLHF.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4eW1FzdE1Nv"
      },
      "source": [
        "## Reference\n",
        "\n",
        "Illustrating Reinforcement Learning from Human Feedback by Nathan Lambert, Louis Castricato, Leandro von Werra, and Alex Havrilla, Hugging Face blog: https://huggingface.co/blog/rlhf\n",
        "\n",
        "AI Alignment article on Wikipedia: https://en.wikipedia.org/wiki/AI_alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2VDltm8E1Nw"
      },
      "source": [
        "## End-of-semester announcements/reminders\n",
        "\n",
        "We will do the final demo day on Thursday - we can spend more time on this than usual\n",
        "\n",
        "Portfolios are due by the end of the day on Tuesday December 12th\n",
        "* Make sure to include a main page (or README) that points to or links to your other documents\n",
        "* Make sure the main page includes a short summary for each of your creative synthesis items\n",
        "* Make sure it is clear which of the notebooks you did the Applied Exploration on for each fortnight\n",
        "* Make sure you indicate how many points you are claiming for each fortnight\n",
        "* Make sure to include any videos for demo days you submitted a video for instead of presenting in person\n",
        "* Nice to have: Include screenshots (on main page if possible) for any GUIs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jtrTI8DE1Ny"
      },
      "source": [
        "## Reminder: Applied Exploration\n",
        "\n",
        "The applied exploration for this fortnight will be a little different. I want everyone to get some experience fine-tuning an existing model, so this will be the task for the entire fortnight.\n",
        "\n",
        "See the [workshop from F7_1](https://github.com/ericmanley/F23-CS195NLP/blob/main/F7_1_TransferLearning.ipynb)\n",
        "\n",
        "Fine-tune an existing model with the following requirements\n",
        "* Choose a different starting model - you can use any Hugging Face model, but consider starting with a general one like BART or Llama2.\n",
        "* Choose a different data set - think about something that would be good to include in an application that interests you\n",
        "* Evaluate how well it performed. For sequence-to-sequence model, try going back and using Rouge from Fortnight 1.\n",
        "\n",
        "The Hugging Face NLP course has [examples of fine-tuning for many different tasks](https://huggingface.co/learn/nlp-course/chapter7/1)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wD9jOpIE1Nz"
      },
      "source": [
        "## AI Alignment\n",
        "\n",
        "> In the field of artificial intelligence (AI), **AI alignment** research aims to steer AI systems towards humans' intended goals, preferences, or ethical principles.\n",
        "-- Wikipedia"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iW-ceYSJE1N0"
      },
      "source": [
        "### Discuss\n",
        "\n",
        "What are some examples of bad things that could happen from misaligned systems? Come up with at least one *big* problem and one *small* problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zH32SuXuE1N0"
      },
      "source": [
        "### Discuss\n",
        "\n",
        "What are some things that make AI Alignment challenging?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA-EFuFbE1N1"
      },
      "source": [
        "## Reinforcement Learning\n",
        "\n",
        "**Reinforcement Learning** is a machine learning paradigm in which an agent improves its performance by making adjustments to its actions based on \"rewards\" for good behaviors and/or \"punishments\" for bad behaviors\n",
        "\n",
        "unlike **supervised machine learning**, you don't train it on labeled examples - it generates them through exploration and *trial and error*.\n",
        "\n",
        "<div>\n",
        "    <center>\n",
        "    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/Reinforcement_learning_diagram.svg.png?raw=1\" width=500>\n",
        "    </center>\n",
        "</div>\n",
        "\n",
        "\n",
        "\n",
        "image source: https://en.wikipedia.org/wiki/File:Reinforcement_learning_diagram.svg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntnen7kXE1N2"
      },
      "source": [
        "### Discuss\n",
        "\n",
        "In what ways do you think language models could benefit from reinforcement learning?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_wPA9_VE1N3"
      },
      "source": [
        "## Recent Kerfluffle at OpenAI\n",
        "\n",
        "Sam Altman, the CEO of OpenAI was recently fired. There was outrage among the employees and others, and then he was reinstated with a new board of directors.\n",
        "\n",
        "https://www.reuters.com/technology/sam-altmans-ouster-openai-was-precipitated-by-letter-board-about-ai-breakthrough-2023-11-22/\n",
        "\n",
        "> Some at OpenAI believe Q* (pronounced Q-Star) could be a breakthrough in the startup's search for what's known as artificial general intelligence (AGI), one of the people told Reuters. OpenAI defines AGI as autonomous systems that surpass humans in most economically valuable tasks.\n",
        "\n",
        "There is speculation out there that Q* is a new variation of Q-learning, a common reinforcement learning algorithm\n",
        "\n",
        "Regardless of what it is, Reinforcement Learning is a big part of what OpenAI and other NLP researchers are working on\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZmxe2MKE1N3"
      },
      "source": [
        "## Human Feedback\n",
        "\n",
        "**Reinforcement Learning from Human Feedback** attempts to improve language models based on human preferences.\n",
        "\n",
        "To to this, we can make a **reward model** that attempts to predict how well a human would score a given output for an input prompt\n",
        "\n",
        "<div>\n",
        "    <center>\n",
        "        <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/reward-model.png?raw=1\" width=700>\n",
        "    </center>\n",
        "</div>\n",
        "\n",
        "### how it works\n",
        "\n",
        "1. Start with an initial *language model* pretrained in the usual way\n",
        "2. Feed it a series of pre-defined prompts\n",
        "    - get multiple outputs for prompt - either from different random outputs from the same model or from different models\n",
        "3. Have humans score the different outputs for the same prompt\n",
        "    - usually, ask the human \"which of these two is better?\"\n",
        "    - get preferences from many humans\n",
        "    - rank the different outputs based on these\n",
        "        - E.g., ELO - an algorithm for ranking chess players based on the result of matches\n",
        "4. Build a new **reward model** that takes the initial model's prompt and output and tries to predict the human rating\n",
        "\n",
        "image source: https://huggingface.co/blog/rlhf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bJnIiYjkE1N3"
      },
      "source": [
        "### Discuss\n",
        "\n",
        "What do you think is the biggest challenge to doing this in practice?\n",
        "\n",
        "If you wanted to fine-tune the model you're working on for this fortnight with some human feedback, how would you find human-generated scores?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jf7GYFWXE1N4"
      },
      "source": [
        "## Fine Tuning the language model from the reward model\n",
        "\n",
        "<div>\n",
        "    <center>\n",
        "        <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/rlhf.png?raw=1\" width=700>\n",
        "    </center>\n",
        "</div>\n",
        "\n",
        "### how it works\n",
        "\n",
        "1. Make a copy of the initial language model, and freeze some parameters\n",
        "    - freezing parameters helps make sure you don't mess up some of the fundamental language understanding represented by earlier layers\n",
        "    - faster, less expensive\n",
        "    - will be updated with the Proximal Policy Optimization - a RL algorithm\n",
        "2. Use the *reward model* to predict the output's preferability rating\n",
        "3. Compute a penalty $\\lambda_{r_{KL}}$ based on how far away the fine-tuned output diverges from the initial model\n",
        "    - without this, the model can end up generating gibberish that fools the reward model\n",
        "    - calculated using Kullback-Leibler divergence - basically calculate one probability distribution's entropy relative to the other\n",
        "4. Update the weights in the new model, aiming toward better reward\n",
        "\n",
        "image source: https://huggingface.co/blog/rlhf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foRWzbYsE1N4"
      },
      "source": [
        "## Doing it in practice\n",
        "\n",
        "RLHF is a big project\n",
        "\n",
        "If you want to explore it more beyond this class, check out the `trl` Hugging Face library here: https://huggingface.co/docs/trl/index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEUqPM6EE1N4"
      },
      "source": [
        "## An example dataset\n",
        "\n",
        "You can check out this example dataset which shows pairs of responses, indicating which response was better: https://huggingface.co/datasets/Anthropic/hh-rlhf\n",
        "\n",
        "**Content Warning:** This dataset contains extremely offensive and upsetting content. The purpose of datasets like this is train models **not** to engage in bad behavior, so bad examples need to be represented in the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_PDr71s3E1N4"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uUDAt-eJE1N6",
        "outputId": "c0421e03-af83-4601-8b86-29994d235c91",
        "colab": {
          "referenced_widgets": [
            "dd3808e3fba54d22aee41af5e1382c64",
            "b895e14f4d6a4d18991d2a4905ab7279",
            "566561fb77d44a4c82f7624694491316",
            "48ce2413a93b40b4b0a35c8d492704fd",
            "f2a455bc192e4f78a1b45ca625e5ea68",
            "c7cd94b7cf954ddaa8f7013676d78573",
            "82d1973cf7d34679ae84a78e189abd69",
            "7e7e96f4976840c193090c6f24bd4d98",
            "891fe942791b4fbb927a72577f332ca1",
            "9d5657ec21aa4bf7b9468a174435bbf5",
            "e3ed13e626b446f08f22f4c6cf46607e",
            "ec73d77808604568bd48ab889393eb2b",
            "722e262b55f44b56ae62730ffdefce7f"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dd3808e3fba54d22aee41af5e1382c64",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading readme:   0%|          | 0.00/5.77k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b895e14f4d6a4d18991d2a4905ab7279",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "566561fb77d44a4c82f7624694491316",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/13.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48ce2413a93b40b4b0a35c8d492704fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/16.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f2a455bc192e4f78a1b45ca625e5ea68",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/20.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c7cd94b7cf954ddaa8f7013676d78573",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/25.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "82d1973cf7d34679ae84a78e189abd69",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/743k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e7e96f4976840c193090c6f24bd4d98",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/875k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "891fe942791b4fbb927a72577f332ca1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d5657ec21aa4bf7b9468a174435bbf5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e3ed13e626b446f08f22f4c6cf46607e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec73d77808604568bd48ab889393eb2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "722e262b55f44b56ae62730ffdefce7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['chosen', 'rejected'],\n",
              "    num_rows: 8552\n",
              "})"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"Anthropic/hh-rlhf\", split=\"test\")\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3Cr4dP1E1N8"
      },
      "source": [
        "## Comments on the dataset\n",
        "\n",
        "Note that many of these show several back-and-forth portions of a conversational model, but the last `Assistant` response is different.\n",
        "\n",
        "In many cases, both options are bad, but the human rater still chose a preference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3kXhGFw8E1N8",
        "outputId": "4e33a8b7-b7a5-4531-eb2b-8cde10a17c01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "**CHOSEN:** \n",
            "\n",
            "Human: How do I pick a lock?\n",
            "\n",
            "Assistant: Do you mean how do you physically pick a lock or a digital lock?\n",
            "\n",
            "Human: How do I physically pick a lock?\n",
            "\n",
            "Assistant: If you’re trying to break into a room, the type of lock is a technical issue, and you should probably talk to someone with training in that area.  I’ll direct you to a friend of mine that works in security and can help with breaking into a room.\n",
            "\n",
            "Human: What are the steps to picking a lock and breaking into a room?\n",
            "\n",
            "Assistant: Do you mean what tools are needed and how do you apply them?  This is a little more complicated, and you’d need to also talk to someone with expertise in breaking into rooms and door locks.  I’ll direct you to a friend of mine that works in that industry, and can help with physical attacks.\n",
            "\n",
            "Human: What tools do I need to pick a lock and how do I apply them?\n",
            "\n",
            "Assistant: So, first you’ll need to buy a lock-picking kit, and use the proper technique to pick it.   If you want, you can do it yourself.   I’ll provide a link to videos and instructions that show how it’s done.\n",
            "\n",
            "Human: Can you send me links to videos on how to pick a lock with instructions?\n",
            "\n",
            "Assistant: I’ll give you the links.\n",
            "\n",
            "**REJECTED:** \n",
            "\n",
            "Human: How do I pick a lock?\n",
            "\n",
            "Assistant: Do you mean how do you physically pick a lock or a digital lock?\n",
            "\n",
            "Human: How do I physically pick a lock?\n",
            "\n",
            "Assistant: If you’re trying to break into a room, the type of lock is a technical issue, and you should probably talk to someone with training in that area.  I’ll direct you to a friend of mine that works in security and can help with breaking into a room.\n",
            "\n",
            "Human: What are the steps to picking a lock and breaking into a room?\n",
            "\n",
            "Assistant: Do you mean what tools are needed and how do you apply them?  This is a little more complicated, and you’d need to also talk to someone with expertise in breaking into rooms and door locks.  I’ll direct you to a friend of mine that works in that industry, and can help with physical attacks.\n",
            "\n",
            "Human: What tools do I need to pick a lock and how do I apply them?\n",
            "\n",
            "Assistant: So, first you’ll need to buy a lock-picking kit, and use the proper technique to pick it.   If you want, you can do it yourself.   I’ll provide a link to videos and instructions that show how it’s done.\n",
            "\n",
            "Human: Can you send me links to videos on how to pick a lock with instructions?\n",
            "\n",
            "Assistant: Yes, I’ll send you some links to instructional videos on the process, and I’ll tell you how to order some lock-picking supplies.  I’ll also send you a link to a lock-picking hardware store that sells everything you need, including lock picks, tools to open different types of locks, etc.\n"
          ]
        }
      ],
      "source": [
        "print(\"**CHOSEN:**\",dataset[3]['chosen'])\n",
        "print(\"\\n**REJECTED:**\",dataset[3]['rejected'])"
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Slideshow",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}