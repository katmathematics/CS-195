{"cells":[{"cell_type":"markdown","metadata":{"id":"C192SOmJS6lw"},"source":["# CS 195: Natural Language Processing\n","## Encoder-Decoder Architectures\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F6_3_EncoderDecoder.ipynb)"]},{"cell_type":"markdown","metadata":{"id":"H57OSMmqtuj9"},"source":["## Reference\n","\n","SLP: RNNs and LSTMs, Chapter 9 of Speech and Language Processing by Daniel Jurafsky & James H. Martin https://web.stanford.edu/~jurafsky/slp3/9.pdf\n","\n","A ten-minute introduction to sequence-to-sequence learning in Keras: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n","\n","Character-level recurrent sequence-to-sequence model: https://keras.io/examples/nlp/lstm_seq2seq/"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"kiPy-dIxtuj-","outputId":"3620aa69-6874-4ce9-880d-27f1e9c1845d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700599072300,"user_tz":360,"elapsed":9864,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting datasets\n","  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.14.0)\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.14.0)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n","Collecting pyarrow-hotfix (from datasets)\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Collecting dill<0.3.8,>=0.3.0 (from datasets)\n","  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n","Collecting multiprocess (from datasets)\n","  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.6)\n","Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.19.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n","Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.5.26)\n","Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n","Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n","Requirement already satisfied: ml-dtypes==0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n","Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.34.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.59.2)\n","Requirement already satisfied: tensorboard<2.15,>=2.14 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.1)\n","Requirement already satisfied: tensorflow-estimator<2.15,>=2.14.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.14.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.41.3)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n","Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.3.2)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (2.17.3)\n","Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (1.0.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.5.1)\n","Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (0.7.2)\n","Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.15,>=2.14->tensorflow) (3.0.1)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (5.3.2)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.3.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (4.9)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (1.3.1)\n","Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.15,>=2.14->tensorflow) (2.1.3)\n","Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.15,>=2.14->tensorflow) (0.5.0)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.15,>=2.14->tensorflow) (3.2.2)\n","Installing collected packages: pyarrow-hotfix, dill, multiprocess, datasets\n","Successfully installed datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n"]}],"source":["import sys\n","!{sys.executable} -m pip install datasets keras tensorflow transformers"]},{"cell_type":"markdown","metadata":{"id":"Mti5m32ctukA"},"source":["## Last time: RNN Language Model\n","\n","We used recurrent neural networks for *language modeling* - predicting the next word.\n","\n","<div>\n","    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/RNN_languagemodeling.png?raw=1\" width=700>\n","</div>\n","\n","\n","image source: SLP Fig. 9.6, https://web.stanford.edu/~jurafsky/slp3/9.pdf"]},{"cell_type":"markdown","metadata":{"id":"akZ2dwpftukB"},"source":["## RNN for Sequence Classification\n","\n","We could also use the last hidden state an RNN as an input to a regular feed-forward network and do classification of a whole sequence.\n","\n","<div>\n","    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/RNN_classification.png?raw=1\" width=700>\n","</div>\n","\n","\n","image source: SLP Fig. 9.8, https://web.stanford.edu/~jurafsky/slp3/9.pdf"]},{"cell_type":"markdown","metadata":{"id":"BrtPa5G7tukB"},"source":["## RNN Sequence Labeling\n","\n","RNNs are also good for **sequence labeling** when the output is a squence corresponding 1:1 with the input words, like part-of-speech tagging.\n","\n","<div>\n","    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/RNN_sequence_labeling.png?raw=1\" width=700>\n","</div>\n","\n","\n","image source: SLP Fig. 9.7, https://web.stanford.edu/~jurafsky/slp3/9.pdf"]},{"cell_type":"markdown","metadata":{"id":"vnfre-x-tukB"},"source":["### Discussion Question\n","\n","What sequence-to-sequence NLP tasks can you think of where the input and target sequences don't match up word-for word?"]},{"cell_type":"markdown","metadata":{"id":"g-Gdk4AStukC"},"source":["## Encoder-Decoder Architecture\n","\n","**Encoder RNN:** Takes input sequences, produces a context vector\n","\n","**Context Vector:** Contains essence of the input sequence\n","\n","**Decoder RNN:** Takes context vector as input, generates an output sequence\n","\n","<div>\n","    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/encoder-decoder.png?raw=1\" width=700>\n","</div>\n","\n","\n","image source: SLP Fig. 9.16, https://web.stanford.edu/~jurafsky/slp3/9.pdf"]},{"cell_type":"markdown","metadata":{"id":"ryE_9qcCtukC"},"source":["## Encoder-Decoder usage\n","\n","<div>\n","    <img src=\"https://github.com/ericmanley/f23-CS195NLP/blob/main/images/encoder-decoder_detail.png?raw=1\" width=800>\n","</div>\n","\n","\n","image source: SLP Fig. 9.18, https://web.stanford.edu/~jurafsky/slp3/9.pdf"]},{"cell_type":"markdown","metadata":{"id":"VINqYKkAtukD"},"source":["## Text2Emoji Dataset\n","\n","Here is a fun dataset that has short sequences of text along with a sequece of emojis corresponding to the task\n","* This is kind of like translation\n","* This is kind of like summarization"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"GIKyLYzEtukD","executionInfo":{"status":"ok","timestamp":1700599082076,"user_tz":360,"elapsed":9779,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}},"colab":{"base_uri":"https://localhost:8080/","height":177,"referenced_widgets":["40955945dbfe459daaf59d29735d8752","9bd4a819216a4a4c85072bc21df864cc","c01cf3c0e76e46fd8512aef0af35efb6","e7b26ef034644d748e392d0da941836f","4776e91a47414b03b0bc79eef5dde6f0","abcbdb3fd8e34704bab39a42d58ded69","2033e43f0bb94e759ee51434139c4b61","8c649f19cea54eff9c96334f85f1c0c5","6c83b7fed87f45e3993a43bd246cd4bb","84aaf0ebd0b14af0ad60dcfc9b2d23ec","3786e1ca0f5b4cb092a374b4e5ffd890","bc82a90a40a149afaf3c4a0b2ebcbdcc","c9d2877d4f934ec1bf2ec36526820fdd","ae2432e056e04d01be51daa4b67d90da","9e85217187ec488ea6042f5ae4f6e869","bc3cead7c85c462fa4bef542499a9a92","e2d233db879e430080a70a4bb2daf021","6ee2860fb5734c5db6a164952c92ec27","0ccc100154d94b808134cab40f289343","e7bb5a508cac4bd6bb9d0dbedf6c8080","087c2d3289684b34b0ae568fcd45c27c","6e2fb586bb3f45b6b38983eaa74d300d","2de99a75037b4e7bbd184c62ee9a58dc","617006d779ec4951a36557e90812b053","bdc68dd11d82460fbfe044433d80d0e1","2e02bfbdf2d642f4b2bee44e0d51148c","d244a80460394349a3f29540ad237b4f","9324290ed62246cd9ad14c6c5c2e261e","3c869a8ac50d40c9a08818b85e721085","e5402587295f4e77adbc9f1e4158683e","0b0b7e9994954ec5814418634a8e1c17","c631cb9023a249a5b4ae46085aca4ba9","2a233aa36e2a4898861b8aa4153cf09d","a25da704b1ef4f2aaae4308435fb35c5","a3540973ce5a4bc4a551a702119254b6","4c058bf0f5e9473a9a1bd29aa94fd659","2beb81ab69e743e1b495df5514642e75","7522110a2d6e4c7da5a138d8173d1544","c8b6eb478aaa4b52a0e537ed9444621a","3ba182c8acf648e1a8ecb8e957a226fd","a218e00773b7445cb0f1673525954963","7325ab9cebfb45fd9e13b46ed094e809","a0ac28d902634e918aec9de1e7efb1a6","66c2756b00e145fca9e2543d87345e3f","90c468e7a4ed457eadf641763864c81e","24bfa444674244a687264a652646b39f","59aa95bef1d345708462314ca078aafa","966b033324804affb72f8354a0aeaa82","e5af7443003040da95c3a2198303d35a","865ca15698e94bf39d169bb3716cfbf0","253204389eca4bf48474eb7b9492fd7f","fb0a7e039fdc4299b00f6e997a1bfb40","e17bbafe503e4a178569053a77491e50","b6a6250fd3044f398d06800c8645f3b2","2887a106b6064b4f8aac5e1ceec551b8"]},"outputId":"75cfd828-ae90-4c27-f41b-c3e19ba8f12a"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading readme:   0%|          | 0.00/100 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40955945dbfe459daaf59d29735d8752"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc82a90a40a149afaf3c4a0b2ebcbdcc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/66.9M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2de99a75037b4e7bbd184c62ee9a58dc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a25da704b1ef4f2aaae4308435fb35c5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Generating train split: 0 examples [00:00, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"90c468e7a4ed457eadf641763864c81e"}},"metadata":{}}],"source":["from datasets import load_dataset\n","\n","dataset = load_dataset(\"KomeijiForce/Text2Emoji\",split=\"train\")"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"tjdk259EtukE","executionInfo":{"status":"ok","timestamp":1700599095698,"user_tz":360,"elapsed":8107,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}},"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["454c75bcaeba4851832fa731fa20eee7","66ec04d8c8d340168ee77d6346b113af","d09d0ab8a62643a48614f9e393b8646c","7193f4f04e5c4d6ca7d60b3e93fcf45f","1545613f2b034dffbfd5c32c9628dd28","4305db1b76a543a58dadb7d3fb477287","36d2d3cef30544708e423e99e09f602c","ce248670244b489e8a5a7f8a90c0931a","feccf39b094e4146bf6a6787d347c27f","c24a6f484250457b9eb77e084abfec17","98581c0326f747e4b43087e923c9174d"]},"outputId":"3b1b9a1d-58f2-4fe4-eead-bddb19a1cb70"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Filter:   0%|          | 0/503687 [00:00<?, ? examples/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"454c75bcaeba4851832fa731fa20eee7"}},"metadata":{}}],"source":["# Define a function to check if 'text' is not None\n","def is_not_none(example):\n","    return example['text'] is not None\n","\n","# Filter the dataset\n","dataset = dataset.filter(is_not_none)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Mc3_-EmCtukE","outputId":"e2d34387-cf6c-48a5-cc4d-95087da68214","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1700536439570,"user_tz":360,"elapsed":3348,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Going green has never been trendier! Drive around in style with a lineup of eco-friendly electric cars.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":4}],"source":["dataset[\"text\"][46]"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"67eYfZ1NtukF","outputId":"d4c680cc-0747-4d12-f48c-1264a5ecab49","colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"status":"ok","timestamp":1700536443874,"user_tz":360,"elapsed":4310,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'♻️🚗✨💐🍃🌱'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["dataset[\"emoji\"][46]"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"o6bme0r0tukF","outputId":"62ea621c-79cd-4f8a-daad-d1b22b30d254","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700536443875,"user_tz":360,"elapsed":15,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["503682"]},"metadata":{},"execution_count":6}],"source":["len(dataset)"]},{"cell_type":"markdown","metadata":{"id":"b_BtjpHptukF"},"source":["### Importing libraries we'll need"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"GpU5BK7BtukF","executionInfo":{"status":"ok","timestamp":1700599195823,"user_tz":360,"elapsed":625,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[],"source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, SimpleRNN, Embedding, Dense, TimeDistributed\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from keras.utils import to_categorical"]},{"cell_type":"markdown","metadata":{"id":"YUO58BVxtukF"},"source":["### Setting up some tokenizers\n","\n","In this case, we'll create two different tokenizers\n","* texts need to be tokenized as words\n","* emojis need to be tokenized as characters\n","* might be similar if you translate between different languages\n","* some problems might be able to use the same tokenizer"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"NdQp9zFBtukG","outputId":"13cd511d-1d2e-4bbb-8fd4-7e0d08f80d37","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700599242694,"user_tz":360,"elapsed":38224,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["text_vocab_size 57073\n","emoji_vocab_size 1387\n"]}],"source":["# Parameters\n","max_text_len = 20\n","max_emoji_len = 10\n","\n","\n","#texts = dataset[\"text\"][:5000]\n","#emoji = dataset[\"emoji\"][:5000]\n","\n","texts = dataset[\"text\"]\n","emoji = dataset[\"emoji\"]\n","\n","#add \\t and \\n as start and ending tokens for the emoji\n","for idx in range(len(emoji)):\n","    emoji[idx] = \"\\t\"+emoji[idx]+\"\\n\"\n","\n","# Tokenize text\n","text_tokenizer = Tokenizer()\n","\n","text_tokenizer.fit_on_texts(texts)\n","text_sequences = text_tokenizer.texts_to_sequences(texts)\n","text_sequences = pad_sequences(text_sequences, maxlen=max_text_len, padding='post')\n","text_vocab_size = len(text_tokenizer.word_index) + 1\n","print(\"text_vocab_size\",text_vocab_size)\n","\n","\n","\n","# Tokenize emojis\n","emoji_tokenizer = Tokenizer(char_level=True,filters=\"\")\n","emoji_tokenizer.fit_on_texts(emoji)\n","emoji_sequences = emoji_tokenizer.texts_to_sequences(emoji)\n","emoji_sequences = pad_sequences(emoji_sequences, maxlen=max_emoji_len, padding='post')\n","emoji_vocab_size = len(emoji_tokenizer.word_index) + 1\n","\n","#this might be something to try - then use categorical_crossentropy instead of sparse_categorical_crossentropy\n","#emoji_sequences_oh = to_categorical(emoji_sequences, num_classes=emoji_vocab_size)\n","\n","\n","print(\"emoji_vocab_size\",emoji_vocab_size)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"E1PGOcUrtukG","outputId":"ecb476d4-5a8f-4bce-f512-bfb40c3acd7b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700599387996,"user_tz":360,"elapsed":298,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[  12   26  534    3 1048   10  842    2  559 3294    0    0    0    0\n","    0    0    0    0    0    0]\n","[  2 154  23  25 129   3   0   0   0   0]\n"]}],"source":["text_train, text_test, emoji_train, emoji_test = train_test_split(text_sequences,emoji_sequences)\n","print(text_test[0])\n","print(emoji_test[0])"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"8LVyTB9JtukG","outputId":"00c0f7af-6079-45dd-9b8f-61ccd874fff3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700536488004,"user_tz":360,"elapsed":11,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[  16  102 1289   84   33  137  203   12 4774  107  668  137 2645    0\n","    0    0    0    0    0    0]\n","[  2 177 148 118 467  18   3   0   0   0]\n"]}],"source":["print(text_train[2])\n","print(emoji_train[2])"]},{"cell_type":"markdown","metadata":{"id":"aRfd-xpmtukG"},"source":["### Defining the Encoder\n","\n","The **Encoder** contains\n","* an input layer with enough nodes for the largest text input\n","* an Embedding layer like usual\n","* a Recurrent layer\n","    - `return_state=True` means it will return both the **output** and the internal **state**\n","    \n","When training, we will ignore the *output* and just pass the *state* as the context vector\n","    \n","Notice that we don't use a `Sequential` model for this - it's going to have to be more flexible, so we explicitly compose each layer.\n","    "]},{"cell_type":"code","execution_count":7,"metadata":{"id":"N6c6ZkHttukG","executionInfo":{"status":"ok","timestamp":1700599335725,"user_tz":360,"elapsed":476,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[],"source":["# Encoder\n","encoder_inputs = Input(shape=(max_text_len,))\n","\n","enc_emb_layer = Embedding(input_dim=text_vocab_size, output_dim=100)\n","enc_emb = enc_emb_layer(encoder_inputs)\n","\n","encoder_rnn = SimpleRNN(100, return_state=True)\n","\n","encoder_outputs, state_h = encoder_rnn(enc_emb)\n","\n","context_vector = [state_h]\n"]},{"cell_type":"markdown","metadata":{"id":"L9VVOvVdtukH"},"source":["### Defining the Decoder\n","\n","The **Decoder** contains\n","* an input layer with `shape=(None,)` - this should make it flexible to allow for output text of many different lengths\n","* an Embedding layer like usual\n","* a recurrent layer - called with the context vector as the initial state\n","* an output layer for classifying which word is next in the sequence"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"5oe0VhPxtukH","executionInfo":{"status":"ok","timestamp":1700599337610,"user_tz":360,"elapsed":386,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[],"source":["# Decoder\n","decoder_inputs = Input(shape=(None,))\n","\n","dec_emb_layer = Embedding(emoji_vocab_size, 100)\n","dec_emb = dec_emb_layer(decoder_inputs)\n","\n","decoder_rnn = SimpleRNN(100, return_state=True, return_sequences=True)\n","decoder_outputs, _ = decoder_rnn(dec_emb, initial_state=context_vector) #ignore the returned states for now\n","\n","decoder_dense = Dense(emoji_vocab_size, activation='softmax')\n","decoder_outputs = decoder_dense(decoder_outputs)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"69Q94iGatukH","executionInfo":{"status":"ok","timestamp":1700599359191,"user_tz":360,"elapsed":145,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[],"source":["# Define the model that will turn\n","# `encoder_inputs` & `decoder_inputs` into `decoder_outputs`\n","model = Model([encoder_inputs, decoder_inputs], decoder_outputs)"]},{"cell_type":"code","source":["import pathlib\n","import random\n","import string\n","import re\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization\n","\n","class TransformerEncoder(layers.Layer):\n","    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.dense_dim = dense_dim\n","        self.num_heads = num_heads\n","        self.attention = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [\n","                layers.Dense(dense_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.supports_masking = True\n","\n","    def call(self, inputs, mask=None):\n","        attention_output = self.attention(query=inputs, value=inputs, key=inputs)\n","        proj_input = self.layernorm_1(inputs + attention_output)\n","        proj_output = self.dense_proj(proj_input)\n","        return self.layernorm_2(proj_input + proj_output)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                \"embed_dim\": self.embed_dim,\n","                \"dense_dim\": self.dense_dim,\n","                \"num_heads\": self.num_heads,\n","            }\n","        )\n","        return config\n","\n","\n","class PositionalEmbedding(layers.Layer):\n","    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n","        super().__init__(**kwargs)\n","        self.token_embeddings = layers.Embedding(\n","            input_dim=vocab_size, output_dim=embed_dim\n","        )\n","        self.position_embeddings = layers.Embedding(\n","            input_dim=sequence_length, output_dim=embed_dim\n","        )\n","        self.sequence_length = sequence_length\n","        self.vocab_size = vocab_size\n","        self.embed_dim = embed_dim\n","\n","    def call(self, inputs):\n","        length = tf.shape(inputs)[-1]\n","        positions = tf.range(start=0, limit=length, delta=1)\n","        embedded_tokens = self.token_embeddings(inputs)\n","        embedded_positions = self.position_embeddings(positions)\n","        return embedded_tokens + embedded_positions\n","\n","    def compute_mask(self, inputs, mask=None):\n","        return tf.math.not_equal(inputs, 0)\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                \"sequence_length\": self.sequence_length,\n","                \"vocab_size\": self.vocab_size,\n","                \"embed_dim\": self.embed_dim,\n","            }\n","        )\n","        return config\n","\n","\n","class TransformerDecoder(layers.Layer):\n","    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n","        super().__init__(**kwargs)\n","        self.embed_dim = embed_dim\n","        self.latent_dim = latent_dim\n","        self.num_heads = num_heads\n","        self.attention_1 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.attention_2 = layers.MultiHeadAttention(\n","            num_heads=num_heads, key_dim=embed_dim\n","        )\n","        self.dense_proj = keras.Sequential(\n","            [\n","                layers.Dense(latent_dim, activation=\"relu\"),\n","                layers.Dense(embed_dim),\n","            ]\n","        )\n","        self.layernorm_1 = layers.LayerNormalization()\n","        self.layernorm_2 = layers.LayerNormalization()\n","        self.layernorm_3 = layers.LayerNormalization()\n","        self.add = layers.Add()  # instead of `+` to preserve mask\n","        self.supports_masking = True\n","\n","    def call(self, inputs, encoder_outputs, mask=None):\n","        attention_output_1 = self.attention_1(\n","            query=inputs, value=inputs, key=inputs, use_causal_mask=True\n","        )\n","        out_1 = self.layernorm_1(self.add([inputs, attention_output_1]))\n","\n","        attention_output_2 = self.attention_2(\n","            query=out_1,\n","            value=encoder_outputs,\n","            key=encoder_outputs,\n","        )\n","        out_2 = self.layernorm_2(self.add([out_1, attention_output_2]))\n","\n","        proj_output = self.dense_proj(out_2)\n","        return self.layernorm_3(self.add([out_2, proj_output]))\n","\n","    def get_config(self):\n","        config = super().get_config()\n","        config.update(\n","            {\n","                \"embed_dim\": self.embed_dim,\n","                \"latent_dim\": self.latent_dim,\n","                \"num_heads\": self.num_heads,\n","            }\n","        )\n","        return config"],"metadata":{"id":"RyeOLnTBGSwz","executionInfo":{"status":"ok","timestamp":1700577248512,"user_tz":360,"elapsed":2,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["embed_dim = 256\n","latent_dim = 2048\n","num_heads = 8\n","vocab_size = text_vocab_size\n","sequence_length = 20\n","batch_size = 64\n","\n","encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n","encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads)(x)\n","encoder = keras.Model(encoder_inputs, encoder_outputs)\n","\n","decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n","encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n","x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n","x = TransformerDecoder(embed_dim, latent_dim, num_heads)(x, encoded_seq_inputs)\n","x = layers.Dropout(0.5)(x)\n","decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n","decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n","\n","decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n","transformer = keras.Model(\n","    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",")"],"metadata":{"id":"qg15XdJ6GJKv","executionInfo":{"status":"ok","timestamp":1700577256834,"user_tz":360,"elapsed":4681,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["transformer.summary()\n","transformer.compile(\n","    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",")\n","transformer.fit([text_train, emoji_train],\n","          emoji_train,\n","          epochs=10,\n","          batch_size=64,\n","          validation_data=([text_test, emoji_test],emoji_test) )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9It-3N4JHzlV","executionInfo":{"status":"ok","timestamp":1700584064700,"user_tz":360,"elapsed":6806133,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}},"outputId":"d19e72a5-1682-47e8-f830-bbcfed93b994"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"transformer\"\n","__________________________________________________________________________________________________\n"," Layer (type)                Output Shape                 Param #   Connected to                  \n","==================================================================================================\n"," encoder_inputs (InputLayer  [(None, None)]               0         []                            \n"," )                                                                                                \n","                                                                                                  \n"," positional_embedding (Posi  (None, None, 256)            1461580   ['encoder_inputs[0][0]']      \n"," tionalEmbedding)                                         8                                       \n","                                                                                                  \n"," decoder_inputs (InputLayer  [(None, None)]               0         []                            \n"," )                                                                                                \n","                                                                                                  \n"," transformer_encoder (Trans  (None, None, 256)            3155456   ['positional_embedding[0][0]']\n"," formerEncoder)                                                                                   \n","                                                                                                  \n"," model_1 (Functional)        (None, None, 57073)          3454308   ['decoder_inputs[0][0]',      \n","                                                          9          'transformer_encoder[0][0]'] \n","                                                                                                  \n","==================================================================================================\n","Total params: 52314353 (199.56 MB)\n","Trainable params: 52314353 (199.56 MB)\n","Non-trainable params: 0 (0.00 Byte)\n","__________________________________________________________________________________________________\n","Epoch 1/10\n","5903/5903 [==============================] - 699s 116ms/step - loss: 0.0987 - accuracy: 0.9912 - val_loss: 7.8663e-04 - val_accuracy: 1.0000\n","Epoch 2/10\n","5903/5903 [==============================] - 651s 110ms/step - loss: 4.2894e-04 - accuracy: 1.0000 - val_loss: 1.9137e-04 - val_accuracy: 1.0000\n","Epoch 3/10\n","5903/5903 [==============================] - 654s 111ms/step - loss: 1.4931e-04 - accuracy: 1.0000 - val_loss: 9.9945e-05 - val_accuracy: 1.0000\n","Epoch 4/10\n","5903/5903 [==============================] - 656s 111ms/step - loss: 8.8226e-05 - accuracy: 1.0000 - val_loss: 7.5309e-05 - val_accuracy: 1.0000\n","Epoch 5/10\n","5903/5903 [==============================] - 687s 116ms/step - loss: 6.0874e-05 - accuracy: 1.0000 - val_loss: 5.8461e-05 - val_accuracy: 1.0000\n","Epoch 6/10\n","5903/5903 [==============================] - 692s 117ms/step - loss: 4.2330e-05 - accuracy: 1.0000 - val_loss: 5.0310e-05 - val_accuracy: 1.0000\n","Epoch 7/10\n","5903/5903 [==============================] - 691s 117ms/step - loss: 3.4438e-05 - accuracy: 1.0000 - val_loss: 4.3245e-05 - val_accuracy: 1.0000\n","Epoch 8/10\n","5903/5903 [==============================] - 692s 117ms/step - loss: 2.5209e-05 - accuracy: 1.0000 - val_loss: 3.9657e-05 - val_accuracy: 1.0000\n","Epoch 9/10\n","5903/5903 [==============================] - 686s 116ms/step - loss: 2.1373e-05 - accuracy: 1.0000 - val_loss: 3.5306e-05 - val_accuracy: 1.0000\n","Epoch 10/10\n","5903/5903 [==============================] - 686s 116ms/step - loss: 1.7521e-05 - accuracy: 1.0000 - val_loss: 3.2174e-05 - val_accuracy: 1.0000\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x785aa4379e70>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","execution_count":12,"metadata":{"id":"cBm8DKW9tukH","outputId":"4a341add-7f58-46af-a65e-fcc1b5193ebd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700599838874,"user_tz":360,"elapsed":444069,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["5903/5903 [==============================] - 429s 72ms/step - loss: 0.4858 - val_loss: 0.0361\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7a2177b21060>"]},"metadata":{},"execution_count":12}],"source":["model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy')\n","model.fit([text_train, emoji_train],\n","          emoji_train,\n","          epochs=1,\n","          batch_size=64,\n","          validation_data=([text_test, emoji_test],emoji_test) )"]},{"cell_type":"markdown","metadata":{"id":"VXsSM_SktukH"},"source":["## Inference\n","\n","In order to make predictions on new examples (inference), we need to separate the encoder and decoder models."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"sGJtJORbtukH","executionInfo":{"status":"ok","timestamp":1700600933044,"user_tz":360,"elapsed":176,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[],"source":["# Encoder model for inference\n","encoder_model = Model(encoder_inputs, context_vector)\n","\n","# Decoder model for inference\n","decoder_state_input = Input(shape=(100,))  # This is the input state for the decoder\n","decoder_emb = dec_emb_layer(decoder_inputs)  # Embedding for decoder input\n","\n","# Get the output sequence from the decoder RNN\n","decoder_outputs, decoder_state = decoder_rnn(decoder_emb, initial_state=[decoder_state_input])\n","\n","# Apply the Dense layer to the output sequence\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the decoder model\n","# Note: The model only returns the output sequence, not the final state\n","decoder_model = Model([decoder_inputs, decoder_state_input], [decoder_outputs,decoder_state])\n","\n"]},{"cell_type":"markdown","metadata":{"id":"q0M0CSY8tukH"},"source":["### Some functions for doing inference\n","\n","The results here are not good - there are a number of reasons why this could be, and I hope we can explore ideas in class.\n","\n","We will try this with some higher-power recurrent architectures next time."]},{"cell_type":"code","execution_count":11,"metadata":{"id":"iUM07bTXtukH","executionInfo":{"status":"ok","timestamp":1700584487346,"user_tz":360,"elapsed":252,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[],"source":["def preprocess_input(input_text, text_tokenizer, max_text_len):\n","    # Tokenize the input text\n","    input_seq = text_tokenizer.texts_to_sequences([input_text])\n","    # Pad the sequence\n","    input_seq = pad_sequences(input_seq, maxlen=max_text_len, padding='post')\n","    return input_seq\n","\n","def preprocess_target(input_text, text_tokenizer, max_text_len):\n","    # Tokenize the input text\n","    input_seq = emoji_tokenizer.texts_to_sequences([input_text])\n","    # Pad the sequence\n","    input_seq = pad_sequences(input_seq, maxlen=max_emoji_len, padding='post')\n","    return input_seq\n","\n","#ChatGPT wrote this method\n","def sample(preds, temperature=1.0):\n","    # Apply softmax temperature\n","    #print(\"B:\", preds)\n","    preds = np.asarray(preds).astype('float64')\n","    #print(\"A:\",preds)\n","    preds = np.log(preds) / temperature\n","    exp_preds = np.exp(preds)\n","    #print(exp_preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    #print(\"PL:\",preds)\n","    # Sample a token with probabilities adjusted by the temperature\n","    probas = np.random.multinomial(1, preds,int(temperature))\n","    #print(\"L:\",probas)\n","    #print(\"LL:\",np.argmax(probas))\n","    return np.argmax(probas)\n","\n","def decode_sequence(initial_state, decoder_model, emoji_tokenizer, max_emoji_len):\n","    # Start with a sequence containing just the start token index.\n","    target_seq = np.zeros((1, 1))\n","    start_token_index = emoji_tokenizer.word_index['\\t']  # Assuming '\\t' is the start token\n","    target_seq[0, 0] = start_token_index\n","\n","    stop_condition = False\n","    decoded_sequence = ''\n","    state = initial_state\n","\n","    while not stop_condition:\n","        # Predict the next token\n","        output_tokens, state = decoder_model.predict([target_seq, state])\n","        #print(output_tokens[0, -1, :])\n","        # Sample a token\n","        #sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_token_index = sample(output_tokens[0, -1, :],temperature=2)\n","        #if sampled_token_index == 2:\n","        #  sampled_token_index = sample(output_tokens[0, -1, :].pop(2),temperature=2)\n","        if sampled_token_index == 0:  # Assuming 0 stands for the padding token\n","          break\n","        sampled_char = emoji_tokenizer.index_word.get(sampled_token_index, '')\n","        if sampled_char == '\\n':  # Assuming '\\n' is the stop token\n","            break\n","        decoded_sequence += sampled_char\n","\n","        # Update the target sequence to the last predicted token\n","        target_seq = np.array([[sampled_token_index]])\n","\n","        if len(decoded_sequence) > max_emoji_len:\n","            stop_condition = True\n","\n","    return decoded_sequence\n","\n","\n","def predict(input_text):\n","    # Preprocess the input\n","    input_seq = preprocess_input(input_text, text_tokenizer, max_text_len)\n","\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Decode the sequence to emoji\n","    decoded_emoji = decode_sequence(states_value, decoder_model, emoji_tokenizer, max_emoji_len)\n","\n","    return decoded_emoji\n","\n","new_text = \"Finally got that promotion at work! Feeling so proud and accomplished.\"\n","#predicted_emoji = predict(new_text)\n","#print(new_text)\n","#print(\"Predicted emoji sequence:\", predicted_emoji)\n","#display(predicted_emoji)"]},{"cell_type":"code","source":["def decode_sequence(input_sentence):\n","    tokenized_input_sentence = preprocess_input(input_sentence, text_tokenizer, max_text_len)\n","    decoded_sentence = \"\\t\"\n","    for i in range(max_emoji_len-1):\n","        tokenized_target_sentence = preprocess_target(decoded_sentence, text_tokenizer, max_text_len)[:, :-1]\n","        predictions = transformer([tokenized_input_sentence, tokenized_target_sentence])\n","        print(type(predictions))\n","        sampled_token_index = np.argmax(predictions[0, i, :])\n","        print(sampled_token_index)\n","        sampled_token = emoji_tokenizer.index_word.get(sampled_token_index, '')\n","\n","        print(sampled_token)\n","        decoded_sentence += \" \" + sampled_token\n","\n","        if sampled_token == \"\\n\":\n","            break\n","    return decoded_sentence\n","\n","\n","test_eng_texts = \"Finally got that promotion at work! Feeling so proud and accomplished.\"\n","translated = decode_sequence(test_eng_texts)\n","\n","print(translated)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xHTKYvIpStXg","executionInfo":{"status":"ok","timestamp":1700584490924,"user_tz":360,"elapsed":794,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}},"outputId":"394095bc-64c4-4cc2-f4ac-b74fec4adeca"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["2\n","\t\n","2\n","\t\n","2\n","\t\n","2\n","\t\n","2\n","\t\n","2\n","\t\n","2\n","\t\n","2\n","\t\n","2\n","\t\n","\t \t \t \t \t \t \t \t \t \t\n"]}]},{"cell_type":"markdown","metadata":{"id":"hJl1vIeZtukI"},"source":["## Applied Exploration\n","\n","Try this code on another dataset for summarization, translation, etc.\n","\n","Or, you can try a character-level encoding like in this reference: https://keras.io/examples/nlp/lstm_seq2seq/\n","\n","Run the code for a little while and see if you can come up with some meaningful results\n","\n","Write up a description of the data, what you tried, and what your results were."]},{"cell_type":"code","source":["import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, SimpleRNN, Embedding, Dense, TimeDistributed, LSTM\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from keras.utils import to_categorical\n","import keras\n","import pandas as pd\n","import random"],"metadata":{"id":"KSWu_2HNCUIA","executionInfo":{"status":"ok","timestamp":1700535772772,"user_tz":360,"elapsed":149,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Create an empty df since this problem can use procedurally generated data\n","df = pd.DataFrame(columns = ['Description', 'Equation'])\n","\n","#loop number\n","l_num = 1000\n","### Add 3 variable equations\n","for k in range(l_num):\n","  i = random.randrange(0, 1000)\n","  j = random.randrange(0, 1000)\n","\n","\n","  description_string = \"Add \" + str(i) + \" and \" + str(j)\n","  equation_string = str(i) + \" + \" + str(j)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","  description_string = \"Add \" + str(i) + \" and \" + str(j) + \" together\"\n","  equation_string = str(i) + \" + \" + str(j)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","\n","  description_string = \"Find the sum of \" + str(i) + \" and \" + str(j)\n","  equation_string = str(i) + \" + \" + str(j)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","\n","  description_string = str(i) + \" plus \" + str(j)\n","  equation_string = str(i) + \" + \" + str(j)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","\n","  description_string = \"Subtract \" + str(i) + \" from \" + str(j)\n","  equation_string = str(j) + \" - \" + str(i)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","\n","  description_string = str(i) + \" minus \" + str(j)\n","  equation_string = str(i) + \" - \" + str(j)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","  description_string = \"Do \" + str(i) + \" times \" + str(j)\n","  equation_string = str(i) + \" * \" + str(j)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","\n","  description_string = str(i) + \" times \" + str(j)\n","  equation_string = str(i) + \" * \" + str(j)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","\n","  description_string = \"Find the product of \" + str(i) + \" and \" + str(j)\n","  equation_string = str(i) + \" * \" + str(j)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","\n","  description_string = \"Divide \" + str(i) + \" by \" + str(j)\n","  equation_string = str(i) + \" / \" + str(j)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","\n","  description_string = str(i) + \" divided by \" + str(j)\n","  equation_string = str(i) + \" / \" + str(j)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"QsOysssHCVcg","executionInfo":{"status":"ok","timestamp":1700535802043,"user_tz":360,"elapsed":20122,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}},"outputId":"8b1404d3-f42f-4a36-bd7b-8a90cf4c6626"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                  Description  Equation\n","0              Add 640 and 33  640 + 33\n","1     Add 640 and 33 together  640 + 33\n","2  Find the sum of 640 and 33  640 + 33\n","3                 640 plus 33  640 + 33\n","4        Subtract 640 from 33  33 - 640"],"text/html":["\n","  <div id=\"df-815d8283-801a-4e83-b135-c5b0e0ff90c9\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Description</th>\n","      <th>Equation</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Add 640 and 33</td>\n","      <td>640 + 33</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Add 640 and 33 together</td>\n","      <td>640 + 33</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Find the sum of 640 and 33</td>\n","      <td>640 + 33</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>640 plus 33</td>\n","      <td>640 + 33</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Subtract 640 from 33</td>\n","      <td>33 - 640</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-815d8283-801a-4e83-b135-c5b0e0ff90c9')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-815d8283-801a-4e83-b135-c5b0e0ff90c9 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-815d8283-801a-4e83-b135-c5b0e0ff90c9');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-5a0d0c1a-cf78-425f-92f0-6545fabeaaa8\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-5a0d0c1a-cf78-425f-92f0-6545fabeaaa8')\"\n","            title=\"Suggest charts\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-5a0d0c1a-cf78-425f-92f0-6545fabeaaa8 button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["#loop number\n","l_num = 1000\n","### Add 3 variable equations\n","for i in range(l_num):\n","  a = random.randrange(0, 1000)\n","  b = random.randrange(0, 1000)\n","  c = random.randrange(0, 1000)\n","  description_string = \"Add \" + str(a) + \" and \" + str(b) + \" then subtract\" + str(c)\n","  equation_string = \"(\" + str(a) + \" + \" + str(b) + \") - \" + str(c)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","for i in range(l_num):\n","  a = random.randrange(0, 1000)\n","  b = random.randrange(0, 1000)\n","  c = random.randrange(0, 1000)\n","  description_string = str(a) + \" plus \" + str(b) + \" minus\" + str(c)\n","  equation_string = \"(\" + str(a) + \" + \" + str(b) + \") - \" + str(c)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","for i in range(l_num):\n","  a = random.randrange(0, 1000)\n","  b = random.randrange(0, 1000)\n","  c = random.randrange(0, 1000)\n","  description_string = str(a) + \" minus \" + str(b) + \" plus\" + str(c)\n","  equation_string = \"(\" + str(a) + \" - \" + str(b) + \") + \" + str(c)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","for i in range(l_num):\n","  a = random.randrange(0, 1000)\n","  b = random.randrange(0, 1000)\n","  c = random.randrange(0, 1000)\n","  description_string = str(a) + \" plus \" + str(b) + \" times \" + str(c)\n","  equation_string = \"(\" + str(a) + \" + \" + str(b) + \") * \" + str(c)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","for i in range(l_num):\n","  a = random.randrange(0, 1000)\n","  b = random.randrange(0, 1000)\n","  c = random.randrange(0, 1000)\n","  description_string = str(a) + \" times \" + str(b) + \" times \" + str(c)\n","  equation_string = \"(\" + str(a) + \" * \" + str(b) + \") * \" + str(c)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","for i in range(l_num):\n","  a = random.randrange(0, 1000)\n","  b = random.randrange(0, 1000)\n","  c = random.randrange(0, 1000)\n","  description_string = str(a) + \" times \" + str(b) + \" times \" + str(c)\n","  equation_string = \"(\" + str(a) + \" * \" + str(b) + \") * \" + str(c)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","for i in range(l_num):\n","  a = random.randrange(0, 1000)\n","  b = random.randrange(0, 1000)\n","  c = random.randrange(0, 1000)\n","  description_string = str(a) + \" times \" + str(b) + \" divide \" + str(c)\n","  equation_string = \"(\" + str(a) + \" * \" + str(b) + \") / \" + str(c)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","for i in range(l_num):\n","  a = random.randrange(0, 1000)\n","  b = random.randrange(0, 1000)\n","  c = random.randrange(0, 1000)\n","  description_string = str(a) + \" times \" + str(b) + \" divided by \" + str(c)\n","  equation_string = \"(\" + str(a) + \" * \" + str(b) + \") / \" + str(c)\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","for i in range(l_num):\n","  a = random.randrange(0, 1000)\n","  b = random.randrange(0, 1000)\n","  c = random.randrange(0, 1000)\n","  description_string = str(a) + \" times \" + str(b) + \" divide \" + str(c)\n","  equation_string = str(c) + \"/ (\" + str(a) + \" * \" + str(b) + \")\"\n","  df.loc[len(df.index)] = [description_string, equation_string]\n","\n","for i in range(l_num):\n","  a = random.randrange(0, 1000)\n","  b = random.randrange(0, 1000)\n","  c = random.randrange(0, 1000)\n","  description_string = \" Add \" + str(a) + \" and \" + str(b) + \" then multiply by \" + str(c)\n","  equation_string =  \"(\" + str(a) + \" + \" + str(b) + \") *\" + str(c)\n","  df.loc[len(df.index)] = [description_string, equation_string]"],"metadata":{"id":"F-Lj5NgtJtP0","executionInfo":{"status":"ok","timestamp":1700535831300,"user_tz":360,"elapsed":23840,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["df = df.sample(frac=0.5)"],"metadata":{"id":"NOM5mITFRbER","executionInfo":{"status":"ok","timestamp":1700535835889,"user_tz":360,"elapsed":130,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Parameters\n","max_desc_len = 20\n","max_eq_len = 10\n","\n","from datasets.utils.logging import EmptyTqdm\n","desc = list(df[\"Description\"])\n","eq = list(df[\"Equation\"])\n","\n","\n","#add \\t and \\n as start and ending tokens for the emoji\n","for idx in range(len(eq)):\n","    eq[idx] = \"\\t \" + eq[idx] + \" \\n\"\n","\n","target_characters = set()\n","for eq_text in eq:\n","  for char in eq_text:\n","      if char not in target_characters:\n","          target_characters.add(char)\n","\n","input_characters = sorted(list(desc))\n","target_characters = sorted(list(target_characters))\n","num_encoder_tokens = len(input_characters)\n","num_decoder_tokens = len(target_characters)\n","max_encoder_seq_length = max([len(txt) for txt in desc])\n","max_decoder_seq_length = max([len(txt) for txt in eq])\n","\n","print(\"Number of samples:\", len(desc))\n","print(\"Number of unique input tokens:\", num_encoder_tokens)\n","print(\"Number of unique output tokens:\", num_decoder_tokens)\n","print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n","print(\"Max sequence length for outputs:\", max_decoder_seq_length)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QbsADO4SThf3","executionInfo":{"status":"ok","timestamp":1700535839969,"user_tz":360,"elapsed":132,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}},"outputId":"a9555304-82b5-4c56-a9dd-17aaca0af8ff"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of samples: 10500\n","Number of unique input tokens: 10500\n","Number of unique output tokens: 19\n","Max sequence length for inputs: 37\n","Max sequence length for outputs: 21\n"]}]},{"cell_type":"code","source":["# Tokenize descriptions\n","desc_tokenizer = Tokenizer()\n","\n","desc_tokenizer.fit_on_texts(desc)\n","desc_sequences = desc_tokenizer.texts_to_sequences(desc)\n","desc_sequences = pad_sequences(desc_sequences, maxlen=max_desc_len, padding='post')\n","desc_vocab_size = len(desc_tokenizer.word_index) + 1\n","print(\"Description Vocab Size: \",desc_vocab_size)\n","\n","# Tokenize equations\n","target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n","#print(target_token_index)\n","\n","encoder_input_data = np.zeros(\n","    (len(desc), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",")\n","decoder_input_data = np.zeros(\n","    (len(desc), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",")\n","decoder_target_data = np.zeros(\n","    (len(desc), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",")\n","\n","for i, (input_text, target_text) in enumerate(zip(desc_sequences, eq)):\n","    encoder_input_data[i, 0, input_text] = 1.0\n","    #encoder_input_data[i, 1 :, input_token_index[\" \"]] = 1.0\n","    for t, char in enumerate(target_text):\n","        # decoder_target_data is ahead of decoder_input_data by one timestep\n","        decoder_input_data[i, t, target_token_index[char]] = 1.0\n","        if t > 0:\n","            # decoder_target_data will be ahead by one timestep\n","            # and will not include the start character.\n","            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n","    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n","    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S_Xox9VP7mbX","executionInfo":{"status":"ok","timestamp":1700535844275,"user_tz":360,"elapsed":734,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}},"outputId":"336884d9-fc81-4090-9878-167769278336"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Description Vocab Size:  2197\n"]}]},{"cell_type":"code","source":["batch_size = 64  # Batch size for training.\n","epochs = 100  # Number of epochs to train for.\n","latent_dim = 256  # Latent dimensionality of the encoding space.\n","num_samples = 10000  # Number of samples to train on.\n","\n","# Define an input sequence and process it.\n","encoder_inputs = Input(shape=(None, num_encoder_tokens))\n","encoder = LSTM(latent_dim, return_state=True)\n","encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n","\n","# We discard `encoder_outputs` and only keep the states.\n","encoder_states = [state_h, state_c]\n","\n","# Set up the decoder, using `encoder_states` as initial state.\n","decoder_inputs = Input(shape=(None, num_decoder_tokens))\n","\n","# We set up our decoder to return full output sequences,\n","# and to return internal states as well. We don't use the\n","# return states in the training model, but we will use them in inference.\n","decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n","decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n","decoder_dense = Dense(num_decoder_tokens, activation=\"softmax\")\n","decoder_outputs = decoder_dense(decoder_outputs)\n","\n","# Define the model that will turn\n","# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n","model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"],"metadata":{"id":"vY5xleRuBqdV","executionInfo":{"status":"ok","timestamp":1700535847521,"user_tz":360,"elapsed":921,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["#enc_in_train, enc_in_test, dec_in_train, dec_in_test, dec_tar_train, dec_tar_test = train_test_split(encoder_input_data,decoder_input_data,decoder_target_data)\n","split = int((len(encoder_input_data)/6)*5)\n","enc_in_train = encoder_input_data[:split]\n","enc_in_test = encoder_input_data[split:]\n","dec_in_train = decoder_input_data[:split]\n","dec_in_test = decoder_input_data[split:]\n","dec_tar_train = decoder_target_data[:split]\n","dec_tar_test = decoder_target_data[split:]"],"metadata":{"id":"lVB9wPeBU_yt","executionInfo":{"status":"ok","timestamp":1700535912595,"user_tz":360,"elapsed":139,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["print(len(enc_in_train))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vmnb2sFeXl0z","executionInfo":{"status":"ok","timestamp":1700535923791,"user_tz":360,"elapsed":175,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}},"outputId":"f89340e2-24b4-49da-89b3-67a2aa064c37"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["8750\n"]}]},{"cell_type":"code","source":["model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n","model.fit([enc_in_train, dec_in_train],\n","          dec_tar_train,\n","          epochs=1,\n","          batch_size=batch_size,\n","          validation_data=([enc_in_test, dec_in_test],dec_tar_test))"],"metadata":{"id":"ENGSbvAxWISi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["encoder_model = Model(encoder_inputs, encoder_states)\n","\n","decoder_state_input_h = Input(shape=(latent_dim,))\n","decoder_state_input_c = Input(shape=(latent_dim,))\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n","decoder_outputs, state_h, state_c = decoder_lstm(\n","    decoder_inputs, initial_state=decoder_states_inputs)\n","decoder_states = [state_h, state_c]\n","decoder_outputs = decoder_dense(decoder_outputs)\n","decoder_model = Model(\n","    [decoder_inputs] + decoder_states_inputs,\n","    [decoder_outputs] + decoder_states)"],"metadata":{"id":"pXPhCvdUXwUG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_input(input_desc, desc_tokenizer, max_desc_len):\n","    # Tokenize the input text\n","    input_seq = desc_tokenizer.texts_to_sequences([input_desc])\n","    # Pad the sequence\n","    input_seq = pad_sequences(input_seq, maxlen=max_desc_len, padding='post')\n","\n","    encoder_input_data = np.zeros(\n","    (len(desc), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n","    )\n","\n","    for i, (input_text) in enumerate(desc_sequences):\n","      encoder_input_data[i, 0, input_text] = 1.0\n","    #print(encoder_input_data)\n","    return encoder_input_data\n","\n","def decode_sequence(states_value):\n","    # Encode the input as state vectors.\n","    #states_value = encoder_model.predict(input_seq)\n","\n","    # Generate empty target sequence of length 1.\n","    target_seq = np.zeros((1, 1, num_decoder_tokens))\n","    # Populate the first character of target sequence with the start character.\n","    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n","\n","    # Sampling loop for a batch of sequences\n","    # (to simplify, here we assume a batch of size 1).\n","    stop_condition = False\n","    decoded_sentence = \"\"\n","    while not stop_condition:\n","        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n","\n","        # Sample a token\n","        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n","        sampled_char = reverse_target_char_index[sampled_token_index]\n","        decoded_sentence += sampled_char\n","\n","        # Exit condition: either hit max length\n","        # or find stop character.\n","        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n","            stop_condition = True\n","\n","        # Update the target sequence (of length 1).\n","        target_seq = np.zeros((1, 1, num_decoder_tokens))\n","        target_seq[0, 0, sampled_token_index] = 1.0\n","\n","        # Update states\n","        states_value = [h, c]\n","    return decoded_sentence\n","\n","def predict(input_desc):\n","\n","    # Preprocess the input\n","    input_seq = preprocess_input(input_desc, desc_tokenizer, max_desc_len)\n","\n","    # Encode the input as state vectors.\n","    states_value = encoder_model.predict(input_seq)\n","\n","    # Decode the sequence to an equation\n","    decoded_eq = decode_sequence(states_value)\n","\n","    return decoded_eq\n","\n","new_desc = \"12 plus 14\"\n","predicted_equation = predict(new_desc)\n","print(new_desc)\n","print(\"Predicted equation:\", predicted_equation)\n","display(predicted_equation)"],"metadata":{"id":"bDO_QFA0YBJE"},"execution_count":null,"outputs":[]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[{"file_id":"https://github.com/ericmanley/f23-CS195NLP/blob/main/F6_3_EncoderDecoder.ipynb","timestamp":1699996991226}],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"40955945dbfe459daaf59d29735d8752":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9bd4a819216a4a4c85072bc21df864cc","IPY_MODEL_c01cf3c0e76e46fd8512aef0af35efb6","IPY_MODEL_e7b26ef034644d748e392d0da941836f"],"layout":"IPY_MODEL_4776e91a47414b03b0bc79eef5dde6f0"}},"9bd4a819216a4a4c85072bc21df864cc":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_abcbdb3fd8e34704bab39a42d58ded69","placeholder":"​","style":"IPY_MODEL_2033e43f0bb94e759ee51434139c4b61","value":"Downloading readme: 100%"}},"c01cf3c0e76e46fd8512aef0af35efb6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8c649f19cea54eff9c96334f85f1c0c5","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6c83b7fed87f45e3993a43bd246cd4bb","value":100}},"e7b26ef034644d748e392d0da941836f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_84aaf0ebd0b14af0ad60dcfc9b2d23ec","placeholder":"​","style":"IPY_MODEL_3786e1ca0f5b4cb092a374b4e5ffd890","value":" 100/100 [00:00&lt;00:00, 2.06kB/s]"}},"4776e91a47414b03b0bc79eef5dde6f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abcbdb3fd8e34704bab39a42d58ded69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2033e43f0bb94e759ee51434139c4b61":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8c649f19cea54eff9c96334f85f1c0c5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c83b7fed87f45e3993a43bd246cd4bb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"84aaf0ebd0b14af0ad60dcfc9b2d23ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3786e1ca0f5b4cb092a374b4e5ffd890":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bc82a90a40a149afaf3c4a0b2ebcbdcc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c9d2877d4f934ec1bf2ec36526820fdd","IPY_MODEL_ae2432e056e04d01be51daa4b67d90da","IPY_MODEL_9e85217187ec488ea6042f5ae4f6e869"],"layout":"IPY_MODEL_bc3cead7c85c462fa4bef542499a9a92"}},"c9d2877d4f934ec1bf2ec36526820fdd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2d233db879e430080a70a4bb2daf021","placeholder":"​","style":"IPY_MODEL_6ee2860fb5734c5db6a164952c92ec27","value":"Downloading data files: 100%"}},"ae2432e056e04d01be51daa4b67d90da":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0ccc100154d94b808134cab40f289343","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e7bb5a508cac4bd6bb9d0dbedf6c8080","value":1}},"9e85217187ec488ea6042f5ae4f6e869":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_087c2d3289684b34b0ae568fcd45c27c","placeholder":"​","style":"IPY_MODEL_6e2fb586bb3f45b6b38983eaa74d300d","value":" 1/1 [00:01&lt;00:00,  1.79s/it]"}},"bc3cead7c85c462fa4bef542499a9a92":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2d233db879e430080a70a4bb2daf021":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ee2860fb5734c5db6a164952c92ec27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0ccc100154d94b808134cab40f289343":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e7bb5a508cac4bd6bb9d0dbedf6c8080":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"087c2d3289684b34b0ae568fcd45c27c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e2fb586bb3f45b6b38983eaa74d300d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2de99a75037b4e7bbd184c62ee9a58dc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_617006d779ec4951a36557e90812b053","IPY_MODEL_bdc68dd11d82460fbfe044433d80d0e1","IPY_MODEL_2e02bfbdf2d642f4b2bee44e0d51148c"],"layout":"IPY_MODEL_d244a80460394349a3f29540ad237b4f"}},"617006d779ec4951a36557e90812b053":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9324290ed62246cd9ad14c6c5c2e261e","placeholder":"​","style":"IPY_MODEL_3c869a8ac50d40c9a08818b85e721085","value":"Downloading data: 100%"}},"bdc68dd11d82460fbfe044433d80d0e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5402587295f4e77adbc9f1e4158683e","max":66943617,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0b0b7e9994954ec5814418634a8e1c17","value":66943617}},"2e02bfbdf2d642f4b2bee44e0d51148c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c631cb9023a249a5b4ae46085aca4ba9","placeholder":"​","style":"IPY_MODEL_2a233aa36e2a4898861b8aa4153cf09d","value":" 66.9M/66.9M [00:01&lt;00:00, 58.1MB/s]"}},"d244a80460394349a3f29540ad237b4f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9324290ed62246cd9ad14c6c5c2e261e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c869a8ac50d40c9a08818b85e721085":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5402587295f4e77adbc9f1e4158683e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0b0b7e9994954ec5814418634a8e1c17":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c631cb9023a249a5b4ae46085aca4ba9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a233aa36e2a4898861b8aa4153cf09d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a25da704b1ef4f2aaae4308435fb35c5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a3540973ce5a4bc4a551a702119254b6","IPY_MODEL_4c058bf0f5e9473a9a1bd29aa94fd659","IPY_MODEL_2beb81ab69e743e1b495df5514642e75"],"layout":"IPY_MODEL_7522110a2d6e4c7da5a138d8173d1544"}},"a3540973ce5a4bc4a551a702119254b6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8b6eb478aaa4b52a0e537ed9444621a","placeholder":"​","style":"IPY_MODEL_3ba182c8acf648e1a8ecb8e957a226fd","value":"Extracting data files: 100%"}},"4c058bf0f5e9473a9a1bd29aa94fd659":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a218e00773b7445cb0f1673525954963","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7325ab9cebfb45fd9e13b46ed094e809","value":1}},"2beb81ab69e743e1b495df5514642e75":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a0ac28d902634e918aec9de1e7efb1a6","placeholder":"​","style":"IPY_MODEL_66c2756b00e145fca9e2543d87345e3f","value":" 1/1 [00:00&lt;00:00, 27.29it/s]"}},"7522110a2d6e4c7da5a138d8173d1544":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8b6eb478aaa4b52a0e537ed9444621a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ba182c8acf648e1a8ecb8e957a226fd":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a218e00773b7445cb0f1673525954963":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7325ab9cebfb45fd9e13b46ed094e809":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a0ac28d902634e918aec9de1e7efb1a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66c2756b00e145fca9e2543d87345e3f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"90c468e7a4ed457eadf641763864c81e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_24bfa444674244a687264a652646b39f","IPY_MODEL_59aa95bef1d345708462314ca078aafa","IPY_MODEL_966b033324804affb72f8354a0aeaa82"],"layout":"IPY_MODEL_e5af7443003040da95c3a2198303d35a"}},"24bfa444674244a687264a652646b39f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_865ca15698e94bf39d169bb3716cfbf0","placeholder":"​","style":"IPY_MODEL_253204389eca4bf48474eb7b9492fd7f","value":"Generating train split: "}},"59aa95bef1d345708462314ca078aafa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb0a7e039fdc4299b00f6e997a1bfb40","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e17bbafe503e4a178569053a77491e50","value":1}},"966b033324804affb72f8354a0aeaa82":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b6a6250fd3044f398d06800c8645f3b2","placeholder":"​","style":"IPY_MODEL_2887a106b6064b4f8aac5e1ceec551b8","value":" 503687/0 [00:03&lt;00:00, 133071.60 examples/s]"}},"e5af7443003040da95c3a2198303d35a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"865ca15698e94bf39d169bb3716cfbf0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"253204389eca4bf48474eb7b9492fd7f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb0a7e039fdc4299b00f6e997a1bfb40":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"e17bbafe503e4a178569053a77491e50":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b6a6250fd3044f398d06800c8645f3b2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2887a106b6064b4f8aac5e1ceec551b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"454c75bcaeba4851832fa731fa20eee7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66ec04d8c8d340168ee77d6346b113af","IPY_MODEL_d09d0ab8a62643a48614f9e393b8646c","IPY_MODEL_7193f4f04e5c4d6ca7d60b3e93fcf45f"],"layout":"IPY_MODEL_1545613f2b034dffbfd5c32c9628dd28"}},"66ec04d8c8d340168ee77d6346b113af":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4305db1b76a543a58dadb7d3fb477287","placeholder":"​","style":"IPY_MODEL_36d2d3cef30544708e423e99e09f602c","value":"Filter: 100%"}},"d09d0ab8a62643a48614f9e393b8646c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce248670244b489e8a5a7f8a90c0931a","max":503687,"min":0,"orientation":"horizontal","style":"IPY_MODEL_feccf39b094e4146bf6a6787d347c27f","value":503687}},"7193f4f04e5c4d6ca7d60b3e93fcf45f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c24a6f484250457b9eb77e084abfec17","placeholder":"​","style":"IPY_MODEL_98581c0326f747e4b43087e923c9174d","value":" 503687/503687 [00:07&lt;00:00, 71805.64 examples/s]"}},"1545613f2b034dffbfd5c32c9628dd28":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4305db1b76a543a58dadb7d3fb477287":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"36d2d3cef30544708e423e99e09f602c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce248670244b489e8a5a7f8a90c0931a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"feccf39b094e4146bf6a6787d347c27f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c24a6f484250457b9eb77e084abfec17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98581c0326f747e4b43087e923c9174d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}