{"cells":[{"cell_type":"markdown","metadata":{"id":"C192SOmJS6lw"},"source":["# CS 195: Natural Language Processing\n","## WordSense Disambiguation\n","\n","[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ericmanley/f23-CS195NLP/blob/main/F4_2_WordSenseDisambiguation.ipynb)\n"]},{"cell_type":"markdown","metadata":{"id":"vJopAfIomMzf"},"source":["## References\n","\n","Word Senses and WordNet, Chapter 23 of *Speech and Language Processing* by Daniel Jurafsky & James H. Martin: https://web.stanford.edu/~jurafsky/slp3/23.pdf\n","\n","WordNet documentation: https://www.nltk.org/api/nltk.corpus.reader.wordnet.html\n","\n","SemCor Corpus Module documentation: https://www.nltk.org/api/nltk.corpus.reader.semcor.html\n","\n","NLTK Stopwords: https://pythonspot.com/nltk-stop-words/\n","\n","Lemmatization with NLTK: https://www.geeksforgeeks.org/python-lemmatization-with-nltk/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"meS1-TCXmMzg","outputId":"31cde387-8cc5-4584-e44f-7b6dd8a00e51","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697142888498,"user_tz":300,"elapsed":11597,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"]}],"source":["import sys\n","!{sys.executable} -m pip install nltk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gzzwOnnsmMzi"},"outputs":[],"source":["#you shouldn't need to do this in Colab, but I had to do it on my own machine\n","#in order to connect to the nltk service\n","import nltk\n","import ssl\n","\n","try:\n","    _create_unverified_https_context = ssl._create_unverified_context\n","except AttributeError:\n","    pass\n","else:\n","    ssl._create_default_https_context = _create_unverified_https_context\n"]},{"cell_type":"markdown","metadata":{"id":"R9lqGgmbmMzj"},"source":["## Word Sense Disambiguation\n","\n","As we explored last time, one word can have many *senses*.\n","\n","The **WordNet** database can be used to look up different word senses of a particular word.\n","\n","The task of figuring out which sense is being usede in a given context is called **word sense disambiguation**\n","\n","Important for\n","* extracting proper meaning from text\n","* translation - e.g., different senses of one word in English might have different translations\n","* question answering"]},{"cell_type":"markdown","metadata":{"id":"i89wm_L_mMzj"},"source":["## Typical approach for WSD\n","\n","Look at the *context* of a word - what other words are around it\n","\n","For example, consider the word **bank** in\n","\n","\"I need to go to the bank and deposit my paycheck.\"\n","\n","We can determine from *deposit*, *paycheck*, and maybe even *go to* that we're talking about a financial institution and not a river bank.\n","\n","Which definition does the context share the most words with?\n","\n","*Definition 1:* 'sloping land (especially the slope beside a body of water)'\n","\n","*Definition 2:* 'a financial institution that accepts deposits and channels the money into lending activities'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"49FZHIhFmMzk","outputId":"f20a518b-1248-438f-b1ff-5c5c4f15d09d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697142922971,"user_tz":300,"elapsed":143,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","2\n"]}],"source":["def compute_overlap(set1, set2):\n","    count_overlap = 0\n","    for item in set1:\n","        if item in set2:\n","            count_overlap += 1\n","    return count_overlap\n","\n","\n","sentence = [\"i\", \"need\", \"to\", \"go\", \"to\", \"the\", \"bank\", \"and\", \"deposit\", \"my\", \"paycheck\"]\n","definition1 = [\"sloping\", \"land\", \"especially\", \"the\", \"slope\", \"beside\", \"a\", \"body\", \"of\", \"water\"]\n","definition2 = [\"a\", \"financial\", \"institution\", \"that\", \"accepts\", \"deposits\", \"and\", \"channels\", \"the\", \"money\", \"into\", \"lending\", \"activities\"]\n","\n","print( compute_overlap(sentence,definition1) )\n","print( compute_overlap(sentence,definition2) )"]},{"cell_type":"markdown","metadata":{"id":"ot4Mmf_1mMzk"},"source":["### Discuss: What problems do you see with this approach?"]},{"cell_type":"markdown","metadata":{"id":"m0hNQTlFmMzl"},"source":["## The Simplified Lesk Algorithm\n","\n","The **Simplified Lesk Algorithm** loops over all possible word senses to find the one whose definition/examples share the most words in common with the sentence context.\n","\n","Given a `word` and `sentence`\n","1. Make a *set* of all the words in the sentence (my need to tokenize)\n","2. Look up all the `synsets` for `word` in **WordNet**\n","3. Loop through the list of `synsets`\n","    * create a signature - the set of all the words that appear the definition and list of examples for this `word` from **WordNet** (may need to tokenize)\n","    * compute the overlap between the signature and the word context\n","    * if this is better than the previous best overlap, save the new sense"]},{"cell_type":"markdown","metadata":{"id":"Ph-RWf0imMzl"},"source":["### Discuss: How should we tokenize our text data for this problem?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tvlS7l3YmMzm","colab":{"base_uri":"https://localhost:8080/","height":104},"executionInfo":{"status":"ok","timestamp":1697145479009,"user_tz":300,"elapsed":186,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}},"outputId":"93d7a441-6359-4e44-dfd3-dc480eecd25d"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["'a flight maneuver; aircraft tips laterally about its longitudinal axis (especially in turning)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":45}],"source":["from nltk.corpus import wordnet as wn\n","import nltk\n","\n","nltk.download(\"punkt\") #need to do this the first time you run it\n","nltk.download('wordnet') #only need to do this once\n","\n","def simplified_lesk(word=\"bank\",sentence=\"I deposit my money at the bank\"):\n","    best_sense = 0\n","\n","    sentence = sentence.lower()\n","    sent_set = set(nltk.word_tokenize(sentence))\n","    word_senses = wn.synsets(word)\n","\n","    highest_similarity = float('-inf')\n","    best_overlap = float('-inf')\n","\n","    for word_sense in word_senses:\n","      count_overlap = 0\n","      for def_word in set(nltk.word_tokenize(word_sense.definition())):\n","        for sent_word in sent_set:\n","          if (sent_word in def_word or def_word in sent_word) and sent_word not in [\"and\",\"the\",\"in\",\"or\",\"but\",\"a\",\"this\",\"that\",\"this is not an efficient way to fix stop words lol\"]:\n","            #count_overlap += (wn.synsets(def_word)).path_similarity(wn.synsets(sent_word))\n","            count_overlap += 1\n","      if best_overlap < count_overlap:\n","        best_overlap = count_overlap\n","        best_sense = word_sense\n","\n","    return best_sense\n","\n","simplified_lesk().definition()"]},{"cell_type":"markdown","metadata":{"id":"XGrJoqGJmMzm"},"source":["### Group Exercise: Finish implementing this algorithm"]},{"cell_type":"markdown","metadata":{"id":"qrjzTjKemMzm"},"source":["## Improving the algorithm\n","\n","Two things we could do to try to improve the Lesk algorithm\n","\n","1. Remove tokens that don't carry meaning like punctuation and *stopwords* (words like \"the\", \"is\", \"to\", etc.)\n","\n","2. Lemmatize the words - convert them into their base form\n","\n","Try to catch the word \"deposit(s)\" in\n","* \"a financial institution that accepts **deposits** and channels the money into lending activities'\n","* \"I need to go to the bank and **deposit** my paycheck.\""]},{"cell_type":"markdown","metadata":{"id":"MP54TFb1mMzm"},"source":["## Stopwords Corpus\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R3zum_i0mMzn","outputId":"fbf9f936-795e-4342-8130-53db4c8b63ac","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697142958976,"user_tz":300,"elapsed":136,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["{'hers', 'having', 'can', 's', \"won't\", \"that'll\", 'before', 'her', \"wouldn't\", 'is', 'for', 'hadn', \"wasn't\", 'ourselves', 'this', 'do', 'who', 'himself', 'the', 'a', 'she', 'themselves', 'against', 'why', 'didn', 'ours', \"should've\", 'about', 'under', 'so', 'needn', 'theirs', 'you', \"aren't\", 'and', 'again', 'don', 'weren', 'doesn', 'yourselves', 'too', \"you'd\", 'between', 'both', 'with', 'my', 'their', 'what', 'mightn', \"hadn't\", 'further', 'very', \"you've\", 'your', \"haven't\", 'haven', 'off', 'them', 'mustn', 'wasn', \"mustn't\", 'above', \"didn't\", 'as', 'being', 'no', 'have', 'until', 'once', 'shouldn', 'are', 'where', 'other', 'we', \"mightn't\", 'i', 'couldn', 'd', 'whom', 'from', \"weren't\", 'those', \"hasn't\", 'nor', 'ma', 'but', 'on', 'not', 'these', 'here', 're', 'y', 've', 'had', 'how', 'll', 'aren', 'be', 'will', \"you'll\", 'that', 'yours', 'an', 'most', \"doesn't\", 'below', 'his', 'herself', 'during', 'hasn', 'each', 'few', 'he', \"isn't\", 'there', 'm', 'while', 'if', 'to', 'now', 'just', 'doing', 'because', 'won', 'some', 'which', 'at', 'by', 'down', \"shouldn't\", 'am', \"you're\", 'isn', 'they', 'in', 'own', 'through', \"couldn't\", \"she's\", 'been', 'then', 'does', 'o', 'of', 'it', \"don't\", 'same', 'me', 'were', 'has', 'only', 'than', 'yourself', 'him', 'itself', 'more', 'shan', 'into', 'any', 'after', 't', 'when', 'its', 'ain', \"needn't\", 'our', 'did', 'up', 'such', 'wouldn', 'was', 'out', 'all', \"it's\", 'or', 'should', 'over', 'myself', \"shan't\"}\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]}],"source":["import nltk\n","from nltk.corpus import stopwords\n","\n","nltk.download('stopwords') #only need to do this once\n","stops = set(stopwords.words('english'))\n","print(stops)"]},{"cell_type":"markdown","metadata":{"id":"gVipFtGymMzn"},"source":["## WordNet Lemmatizer"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"StTujvLEmMzn","outputId":"6e0668f8-3c76-4bcc-9062-ae43853f427a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697142967011,"user_tz":300,"elapsed":4042,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["deposit: deposit\n","deposits: deposit\n"]}],"source":["from nltk.stem import WordNetLemmatizer\n","#nltk.download('wordnet') #do it once\n","\n","lemmatizer = WordNetLemmatizer()\n","\n","print(\"deposit:\", lemmatizer.lemmatize(\"deposit\"))\n","print(\"deposits:\", lemmatizer.lemmatize(\"deposits\"))"]},{"cell_type":"markdown","metadata":{"id":"9PahzaP2mMzn"},"source":["## Exercise\n","\n","Add stopword removal and lemmatization to your Lesk Algorithm implementation."]},{"cell_type":"code","source":["from nltk.corpus import wordnet as wn\n","import nltk\n","\n","nltk.download(\"punkt\") #need to do this the first time you run it\n","nltk.download('wordnet') #only need to do this once\n","\n","def simplified_lesk(word=\"bank\",sentence=\"I deposit my money at the bank\"):\n","    best_sense = 0\n","\n","    #sentence = sentence.lower()\n","    #sent_set = set(nltk.word_tokenize(sentence))\n","    sent_set = set(sentence)\n","    for stop_word in set(stopwords.words('english')):\n","      if stop_word in sent_set:\n","        sent_set.remove(stop_word)\n","    word_senses = wn.synsets(word)\n","\n","    highest_similarity = float('-inf')\n","    best_overlap = float('-inf')\n","\n","    for word_sense in word_senses:\n","      count_overlap = 0\n","      for def_word in set(nltk.word_tokenize(word_sense.definition())):\n","        if def_word not in sent_set:\n","          def_word = lemmatizer.lemmatize(def_word)\n","          for sent_word in sent_set:\n","            sent_word = lemmatizer.lemmatize(sent_word)\n","            if sent_word == def_word:\n","              count_overlap += 1\n","        if best_overlap < count_overlap:\n","          best_overlap = count_overlap\n","          best_sense = word_sense\n","\n","    return best_sense\n","\n","simplified_lesk().definition()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":104},"id":"zii9F0fIxnkl","executionInfo":{"status":"ok","timestamp":1697146982950,"user_tz":300,"elapsed":288,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}},"outputId":"67b60295-59a4-4858-8d35-1d728bb9c632"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["'sloping land (especially the slope beside a body of water)'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":63}]},{"cell_type":"markdown","metadata":{"id":"9dFSaalWmMzo"},"source":["## Dataset for evaluation WSD\n","\n","The SemCor NLTK corpus contains text that has been tagged with WordNet sense (mostly Lemmas)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrpcHn3_mMzo","outputId":"f770c771-a4f7-41aa-dbd9-b8c5c1620b17","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697142980104,"user_tz":300,"elapsed":346,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package semcor to /root/nltk_data...\n"]}],"source":["import nltk\n","nltk.download('semcor') #do this once\n","from nltk.corpus import semcor"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"on_miW_JmMzo","outputId":"a2c4ade9-67ab-4465-e4c0-0dda0257ee52","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697142982319,"user_tz":300,"elapsed":145,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['brown1/tagfiles/br-a01.xml', 'brown1/tagfiles/br-a02.xml', 'brown1/tagfiles/br-a11.xml', 'brown1/tagfiles/br-a12.xml', 'brown1/tagfiles/br-a13.xml', 'brown1/tagfiles/br-a14.xml', 'brown1/tagfiles/br-a15.xml', 'brown1/tagfiles/br-b13.xml', 'brown1/tagfiles/br-b20.xml', 'brown1/tagfiles/br-c01.xml', 'brown1/tagfiles/br-c02.xml', 'brown1/tagfiles/br-c04.xml', 'brown1/tagfiles/br-d01.xml', 'brown1/tagfiles/br-d02.xml', 'brown1/tagfiles/br-d03.xml', 'brown1/tagfiles/br-d04.xml', 'brown1/tagfiles/br-e01.xml', 'brown1/tagfiles/br-e02.xml', 'brown1/tagfiles/br-e04.xml', 'brown1/tagfiles/br-e21.xml', 'brown1/tagfiles/br-e24.xml', 'brown1/tagfiles/br-e29.xml', 'brown1/tagfiles/br-f03.xml', 'brown1/tagfiles/br-f10.xml', 'brown1/tagfiles/br-f19.xml', 'brown1/tagfiles/br-f43.xml', 'brown1/tagfiles/br-g01.xml', 'brown1/tagfiles/br-g11.xml', 'brown1/tagfiles/br-g15.xml', 'brown1/tagfiles/br-h01.xml', 'brown1/tagfiles/br-j01.xml', 'brown1/tagfiles/br-j02.xml', 'brown1/tagfiles/br-j03.xml', 'brown1/tagfiles/br-j04.xml', 'brown1/tagfiles/br-j05.xml', 'brown1/tagfiles/br-j06.xml', 'brown1/tagfiles/br-j07.xml', 'brown1/tagfiles/br-j08.xml', 'brown1/tagfiles/br-j09.xml', 'brown1/tagfiles/br-j10.xml', 'brown1/tagfiles/br-j11.xml', 'brown1/tagfiles/br-j12.xml', 'brown1/tagfiles/br-j13.xml', 'brown1/tagfiles/br-j14.xml', 'brown1/tagfiles/br-j15.xml', 'brown1/tagfiles/br-j16.xml', 'brown1/tagfiles/br-j17.xml', 'brown1/tagfiles/br-j18.xml', 'brown1/tagfiles/br-j19.xml', 'brown1/tagfiles/br-j20.xml', 'brown1/tagfiles/br-j22.xml', 'brown1/tagfiles/br-j23.xml', 'brown1/tagfiles/br-j37.xml', 'brown1/tagfiles/br-j52.xml', 'brown1/tagfiles/br-j53.xml', 'brown1/tagfiles/br-j54.xml', 'brown1/tagfiles/br-j55.xml', 'brown1/tagfiles/br-j56.xml', 'brown1/tagfiles/br-j57.xml', 'brown1/tagfiles/br-j58.xml', 'brown1/tagfiles/br-j59.xml', 'brown1/tagfiles/br-j60.xml', 'brown1/tagfiles/br-j70.xml', 'brown1/tagfiles/br-k01.xml', 'brown1/tagfiles/br-k02.xml', 'brown1/tagfiles/br-k03.xml', 'brown1/tagfiles/br-k04.xml', 'brown1/tagfiles/br-k05.xml', 'brown1/tagfiles/br-k06.xml', 'brown1/tagfiles/br-k07.xml', 'brown1/tagfiles/br-k08.xml', 'brown1/tagfiles/br-k09.xml', 'brown1/tagfiles/br-k10.xml', 'brown1/tagfiles/br-k11.xml', 'brown1/tagfiles/br-k12.xml', 'brown1/tagfiles/br-k13.xml', 'brown1/tagfiles/br-k14.xml', 'brown1/tagfiles/br-k15.xml', 'brown1/tagfiles/br-k16.xml', 'brown1/tagfiles/br-k17.xml', 'brown1/tagfiles/br-k18.xml', 'brown1/tagfiles/br-k19.xml', 'brown1/tagfiles/br-k20.xml', 'brown1/tagfiles/br-k21.xml', 'brown1/tagfiles/br-k22.xml', 'brown1/tagfiles/br-k23.xml', 'brown1/tagfiles/br-k24.xml', 'brown1/tagfiles/br-k25.xml', 'brown1/tagfiles/br-k26.xml', 'brown1/tagfiles/br-k27.xml', 'brown1/tagfiles/br-k28.xml', 'brown1/tagfiles/br-k29.xml', 'brown1/tagfiles/br-l11.xml', 'brown1/tagfiles/br-l12.xml', 'brown1/tagfiles/br-m01.xml', 'brown1/tagfiles/br-m02.xml', 'brown1/tagfiles/br-n05.xml', 'brown1/tagfiles/br-p01.xml', 'brown1/tagfiles/br-r05.xml', 'brown1/tagfiles/br-r06.xml', 'brown1/tagfiles/br-r07.xml', 'brown1/tagfiles/br-r08.xml', 'brown1/tagfiles/br-r09.xml', 'brown2/tagfiles/br-e22.xml', 'brown2/tagfiles/br-e23.xml', 'brown2/tagfiles/br-e25.xml', 'brown2/tagfiles/br-e26.xml', 'brown2/tagfiles/br-e27.xml', 'brown2/tagfiles/br-e28.xml', 'brown2/tagfiles/br-e30.xml', 'brown2/tagfiles/br-e31.xml', 'brown2/tagfiles/br-f08.xml', 'brown2/tagfiles/br-f13.xml', 'brown2/tagfiles/br-f14.xml', 'brown2/tagfiles/br-f15.xml', 'brown2/tagfiles/br-f16.xml', 'brown2/tagfiles/br-f17.xml', 'brown2/tagfiles/br-f18.xml', 'brown2/tagfiles/br-f20.xml', 'brown2/tagfiles/br-f21.xml', 'brown2/tagfiles/br-f22.xml', 'brown2/tagfiles/br-f23.xml', 'brown2/tagfiles/br-f24.xml', 'brown2/tagfiles/br-f25.xml', 'brown2/tagfiles/br-f33.xml', 'brown2/tagfiles/br-f44.xml', 'brown2/tagfiles/br-g12.xml', 'brown2/tagfiles/br-g14.xml', 'brown2/tagfiles/br-g16.xml', 'brown2/tagfiles/br-g17.xml', 'brown2/tagfiles/br-g18.xml', 'brown2/tagfiles/br-g19.xml', 'brown2/tagfiles/br-g20.xml', 'brown2/tagfiles/br-g21.xml', 'brown2/tagfiles/br-g22.xml', 'brown2/tagfiles/br-g23.xml', 'brown2/tagfiles/br-g28.xml', 'brown2/tagfiles/br-g31.xml', 'brown2/tagfiles/br-g39.xml', 'brown2/tagfiles/br-g43.xml', 'brown2/tagfiles/br-g44.xml', 'brown2/tagfiles/br-h09.xml', 'brown2/tagfiles/br-h11.xml', 'brown2/tagfiles/br-h12.xml', 'brown2/tagfiles/br-h13.xml', 'brown2/tagfiles/br-h14.xml', 'brown2/tagfiles/br-h15.xml', 'brown2/tagfiles/br-h16.xml', 'brown2/tagfiles/br-h17.xml', 'brown2/tagfiles/br-h18.xml', 'brown2/tagfiles/br-h21.xml', 'brown2/tagfiles/br-h24.xml', 'brown2/tagfiles/br-j29.xml', 'brown2/tagfiles/br-j30.xml', 'brown2/tagfiles/br-j31.xml', 'brown2/tagfiles/br-j32.xml', 'brown2/tagfiles/br-j33.xml', 'brown2/tagfiles/br-j34.xml', 'brown2/tagfiles/br-j35.xml', 'brown2/tagfiles/br-j38.xml', 'brown2/tagfiles/br-j41.xml', 'brown2/tagfiles/br-j42.xml', 'brown2/tagfiles/br-l08.xml', 'brown2/tagfiles/br-l09.xml', 'brown2/tagfiles/br-l10.xml', 'brown2/tagfiles/br-l13.xml', 'brown2/tagfiles/br-l14.xml', 'brown2/tagfiles/br-l15.xml', 'brown2/tagfiles/br-l16.xml', 'brown2/tagfiles/br-l17.xml', 'brown2/tagfiles/br-l18.xml', 'brown2/tagfiles/br-n09.xml', 'brown2/tagfiles/br-n10.xml', 'brown2/tagfiles/br-n11.xml', 'brown2/tagfiles/br-n12.xml', 'brown2/tagfiles/br-n14.xml', 'brown2/tagfiles/br-n15.xml', 'brown2/tagfiles/br-n16.xml', 'brown2/tagfiles/br-n17.xml', 'brown2/tagfiles/br-n20.xml', 'brown2/tagfiles/br-p07.xml', 'brown2/tagfiles/br-p09.xml', 'brown2/tagfiles/br-p10.xml', 'brown2/tagfiles/br-p12.xml', 'brown2/tagfiles/br-p24.xml', 'brown2/tagfiles/br-r04.xml', 'brownv/tagfiles/br-a03.xml', 'brownv/tagfiles/br-a04.xml', 'brownv/tagfiles/br-a05.xml', 'brownv/tagfiles/br-a06.xml', 'brownv/tagfiles/br-a07.xml', 'brownv/tagfiles/br-a08.xml', 'brownv/tagfiles/br-a09.xml', 'brownv/tagfiles/br-a10.xml', 'brownv/tagfiles/br-a16.xml', 'brownv/tagfiles/br-a17.xml', 'brownv/tagfiles/br-a18.xml', 'brownv/tagfiles/br-a19.xml', 'brownv/tagfiles/br-a20.xml', 'brownv/tagfiles/br-a21.xml', 'brownv/tagfiles/br-a22.xml', 'brownv/tagfiles/br-a23.xml', 'brownv/tagfiles/br-a24.xml', 'brownv/tagfiles/br-a25.xml', 'brownv/tagfiles/br-a26.xml', 'brownv/tagfiles/br-a27.xml', 'brownv/tagfiles/br-a28.xml', 'brownv/tagfiles/br-a29.xml', 'brownv/tagfiles/br-a30.xml', 'brownv/tagfiles/br-a31.xml', 'brownv/tagfiles/br-a32.xml', 'brownv/tagfiles/br-a33.xml', 'brownv/tagfiles/br-a34.xml', 'brownv/tagfiles/br-a35.xml', 'brownv/tagfiles/br-a36.xml', 'brownv/tagfiles/br-a37.xml', 'brownv/tagfiles/br-a38.xml', 'brownv/tagfiles/br-a39.xml', 'brownv/tagfiles/br-a40.xml', 'brownv/tagfiles/br-a41.xml', 'brownv/tagfiles/br-a42.xml', 'brownv/tagfiles/br-a43.xml', 'brownv/tagfiles/br-a44.xml', 'brownv/tagfiles/br-b01.xml', 'brownv/tagfiles/br-b02.xml', 'brownv/tagfiles/br-b03.xml', 'brownv/tagfiles/br-b04.xml', 'brownv/tagfiles/br-b05.xml', 'brownv/tagfiles/br-b06.xml', 'brownv/tagfiles/br-b07.xml', 'brownv/tagfiles/br-b08.xml', 'brownv/tagfiles/br-b09.xml', 'brownv/tagfiles/br-b10.xml', 'brownv/tagfiles/br-b11.xml', 'brownv/tagfiles/br-b12.xml', 'brownv/tagfiles/br-b14.xml', 'brownv/tagfiles/br-b15.xml', 'brownv/tagfiles/br-b16.xml', 'brownv/tagfiles/br-b17.xml', 'brownv/tagfiles/br-b18.xml', 'brownv/tagfiles/br-b19.xml', 'brownv/tagfiles/br-b21.xml', 'brownv/tagfiles/br-b22.xml', 'brownv/tagfiles/br-b23.xml', 'brownv/tagfiles/br-b24.xml', 'brownv/tagfiles/br-b25.xml', 'brownv/tagfiles/br-b26.xml', 'brownv/tagfiles/br-b27.xml', 'brownv/tagfiles/br-c03.xml', 'brownv/tagfiles/br-c05.xml', 'brownv/tagfiles/br-c06.xml', 'brownv/tagfiles/br-c07.xml', 'brownv/tagfiles/br-c08.xml', 'brownv/tagfiles/br-c09.xml', 'brownv/tagfiles/br-c10.xml', 'brownv/tagfiles/br-c11.xml', 'brownv/tagfiles/br-c12.xml', 'brownv/tagfiles/br-c13.xml', 'brownv/tagfiles/br-c14.xml', 'brownv/tagfiles/br-c15.xml', 'brownv/tagfiles/br-c16.xml', 'brownv/tagfiles/br-c17.xml', 'brownv/tagfiles/br-d05.xml', 'brownv/tagfiles/br-d06.xml', 'brownv/tagfiles/br-d07.xml', 'brownv/tagfiles/br-d08.xml', 'brownv/tagfiles/br-d09.xml', 'brownv/tagfiles/br-d10.xml', 'brownv/tagfiles/br-d11.xml', 'brownv/tagfiles/br-d12.xml', 'brownv/tagfiles/br-d13.xml', 'brownv/tagfiles/br-d14.xml', 'brownv/tagfiles/br-d15.xml', 'brownv/tagfiles/br-d16.xml', 'brownv/tagfiles/br-d17.xml', 'brownv/tagfiles/br-e03.xml', 'brownv/tagfiles/br-e05.xml', 'brownv/tagfiles/br-e06.xml', 'brownv/tagfiles/br-e07.xml', 'brownv/tagfiles/br-e08.xml', 'brownv/tagfiles/br-e09.xml', 'brownv/tagfiles/br-e10.xml', 'brownv/tagfiles/br-e11.xml', 'brownv/tagfiles/br-e12.xml', 'brownv/tagfiles/br-e13.xml', 'brownv/tagfiles/br-e14.xml', 'brownv/tagfiles/br-e15.xml', 'brownv/tagfiles/br-e16.xml', 'brownv/tagfiles/br-e17.xml', 'brownv/tagfiles/br-e18.xml', 'brownv/tagfiles/br-e19.xml', 'brownv/tagfiles/br-e20.xml', 'brownv/tagfiles/br-f01.xml', 'brownv/tagfiles/br-f02.xml', 'brownv/tagfiles/br-f04.xml', 'brownv/tagfiles/br-f05.xml', 'brownv/tagfiles/br-f06.xml', 'brownv/tagfiles/br-f07.xml', 'brownv/tagfiles/br-f09.xml', 'brownv/tagfiles/br-f11.xml', 'brownv/tagfiles/br-f12.xml', 'brownv/tagfiles/br-g02.xml', 'brownv/tagfiles/br-g03.xml', 'brownv/tagfiles/br-g04.xml', 'brownv/tagfiles/br-g05.xml', 'brownv/tagfiles/br-g06.xml', 'brownv/tagfiles/br-g07.xml', 'brownv/tagfiles/br-g08.xml', 'brownv/tagfiles/br-g09.xml', 'brownv/tagfiles/br-g10.xml', 'brownv/tagfiles/br-g13.xml', 'brownv/tagfiles/br-h02.xml', 'brownv/tagfiles/br-h03.xml', 'brownv/tagfiles/br-h04.xml', 'brownv/tagfiles/br-h05.xml', 'brownv/tagfiles/br-h06.xml', 'brownv/tagfiles/br-h07.xml', 'brownv/tagfiles/br-h08.xml', 'brownv/tagfiles/br-h10.xml', 'brownv/tagfiles/br-j21.xml', 'brownv/tagfiles/br-j24.xml', 'brownv/tagfiles/br-j25.xml', 'brownv/tagfiles/br-j26.xml', 'brownv/tagfiles/br-j27.xml', 'brownv/tagfiles/br-j28.xml', 'brownv/tagfiles/br-l01.xml', 'brownv/tagfiles/br-l02.xml', 'brownv/tagfiles/br-l03.xml', 'brownv/tagfiles/br-l04.xml', 'brownv/tagfiles/br-l05.xml', 'brownv/tagfiles/br-l06.xml', 'brownv/tagfiles/br-l07.xml', 'brownv/tagfiles/br-m03.xml', 'brownv/tagfiles/br-m04.xml', 'brownv/tagfiles/br-m05.xml', 'brownv/tagfiles/br-m06.xml', 'brownv/tagfiles/br-n01.xml', 'brownv/tagfiles/br-n02.xml', 'brownv/tagfiles/br-n03.xml', 'brownv/tagfiles/br-n04.xml', 'brownv/tagfiles/br-n06.xml', 'brownv/tagfiles/br-n07.xml', 'brownv/tagfiles/br-n08.xml', 'brownv/tagfiles/br-p02.xml', 'brownv/tagfiles/br-p03.xml', 'brownv/tagfiles/br-p04.xml', 'brownv/tagfiles/br-p05.xml', 'brownv/tagfiles/br-p06.xml', 'brownv/tagfiles/br-p08.xml', 'brownv/tagfiles/br-r01.xml', 'brownv/tagfiles/br-r02.xml', 'brownv/tagfiles/br-r03.xml']\n"]}],"source":["# Get a list of file identifiers in SemCor\n","file_ids = semcor.fileids()\n","print(file_ids) #looks like they're from the brown dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jj7_TqySmMzo","outputId":"79b0931a-55da-410b-f9cb-a941bb09ac59","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697142983951,"user_tz":300,"elapsed":138,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', 'Atlanta', \"'s\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'], ['The', 'jury', 'further', 'said', 'in', 'term', 'end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'], ...]\n"]}],"source":["# Access the sense-tagged sentences from a file\n","sentences = semcor.sents(file_ids[0])\n","print(sentences)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PK1vheaBmMzo","outputId":"01a0f77b-d98e-42cc-9034-b9c3dd245948","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697142985550,"user_tz":300,"elapsed":288,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["[[['The'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['Fulton', 'County', 'Grand', 'Jury'])]), Tree(Lemma('state.v.01.say'), ['said']), Tree(Lemma('friday.n.01.Friday'), ['Friday']), ['an'], Tree(Lemma('probe.n.01.investigation'), ['investigation']), ['of'], Tree(Lemma('atlanta.n.01.Atlanta'), ['Atlanta']), [\"'s\"], Tree(Lemma('late.s.03.recent'), ['recent']), Tree(Lemma('primary.n.01.primary_election'), ['primary', 'election']), Tree(Lemma('produce.v.04.produce'), ['produced']), ['``'], ['no'], Tree(Lemma('evidence.n.01.evidence'), ['evidence']), [\"''\"], ['that'], ['any'], Tree(Lemma('abnormality.n.04.irregularity'), ['irregularities']), Tree(Lemma('happen.v.01.take_place'), ['took', 'place']), ['.']], [['The'], Tree(Lemma('jury.n.01.jury'), ['jury']), Tree(Lemma('far.r.02.far'), ['further']), Tree(Lemma('state.v.01.say'), ['said']), ['in'], Tree(Lemma('term.n.02.term'), ['term']), Tree(Lemma('end.n.02.end'), ['end']), Tree(Lemma('presentment.n.01.presentment'), ['presentments']), ['that'], ['the'], Tree(Lemma('group.n.01.group'), [Tree('NE', ['City', 'Executive', 'Committee'])]), [','], ['which'], Tree(Lemma('own.v.01.have'), ['had']), Tree(Lemma('overall.s.02.overall'), ['over-all']), Tree(Lemma('mission.n.03.charge'), ['charge']), ['of'], ['the'], Tree(Lemma('election.n.01.election'), ['election']), [','], ['``'], Tree(Lemma('deserve.v.01.deserve'), ['deserves']), ['the'], Tree(Lemma('praise.n.01.praise'), ['praise']), ['and'], Tree(Lemma('thanks.n.01.thanks'), ['thanks']), ['of'], ['the'], Tree(Lemma('location.n.01.location'), [Tree('NE', ['City', 'of', 'Atlanta'])]), [\"''\"], ['for'], ['the'], Tree(Lemma('manner.n.01.manner'), ['manner']), ['in'], ['which'], ['the'], Tree(Lemma('election.n.01.election'), ['election']), ['was'], Tree(Lemma('conduct.v.01.conduct'), ['conducted']), ['.']], ...]\n"]}],"source":["# Access the sense tags for those sentences\n","tags = semcor.tagged_sents(file_ids[0],tag=\"sem\")\n","print(tags)"]},{"cell_type":"markdown","metadata":{"id":"0ZishbJFmMzp"},"source":["This is a complex format - notice that some (but not all!) of the words are grouped together in a tree structure."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Rb7gW0ymMzp","outputId":"8a40ee99-f0da-4a38-9e64-77ff589a84a7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697142987011,"user_tz":300,"elapsed":126,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["['The']\n","(Lemma('group.n.01.group') (NE Fulton County Grand Jury))\n","(Lemma('state.v.01.say') said)\n","(Lemma('friday.n.01.Friday') Friday)\n","['an']\n","(Lemma('probe.n.01.investigation') investigation)\n","['of']\n","(Lemma('atlanta.n.01.Atlanta') Atlanta)\n","[\"'s\"]\n","(Lemma('late.s.03.recent') recent)\n","(Lemma('primary.n.01.primary_election') primary election)\n","(Lemma('produce.v.04.produce') produced)\n","['``']\n","['no']\n","(Lemma('evidence.n.01.evidence') evidence)\n","[\"''\"]\n","['that']\n","['any']\n","(Lemma('abnormality.n.04.irregularity') irregularities)\n","(Lemma('happen.v.01.take_place') took place)\n","['.']\n"]}],"source":["# tags[0] is the tags for the first sentence, sentence[0]\n","for tag in tags[0]:\n","    print(tag)"]},{"cell_type":"markdown","metadata":{"id":"02pjBIM8mMzp"},"source":["Notice\n","* Some tokens don't have a tag - stopwords, punctuation, etc. - these show up as a string inside a list\n","* \"Fulton County Grand Jury\" is grouped under Lemma('group.n.01.group')\n","* \"primary election\" is grouped as a compound word with Lemma('primary.n.01.primary_election')\n","\n","This is going to be tough to work with. Here's an attempt to loop through them, match them up wit the word from the sentence, and handle these issues."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PMqd68BDmMzp","outputId":"43d6cfaa-cd56-4d41-8ed8-ce96ad2eba21","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1697147045327,"user_tz":300,"elapsed":126,"user":{"displayName":"Katja Mathesius","userId":"05730695603903920499"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Word: The\n","Tag: ['The']\n","\n","Word: Fulton\n","Tag: (Lemma('group.n.01.group') (NE Fulton County Grand Jury))\n","Words in this group: ['Fulton', 'County', 'Grand', 'Jury']\n","\n","Word: said\n","Tag: (Lemma('state.v.01.say') said)\n","Hit!\n","\n","Word: Friday\n","Tag: (Lemma('friday.n.01.Friday') Friday)\n","Hit!\n","\n","Word: an\n","Tag: ['an']\n","\n","Word: investigation\n","Tag: (Lemma('probe.n.01.investigation') investigation)\n","Hit!\n","\n","Word: of\n","Tag: ['of']\n","\n","Word: Atlanta\n","Tag: (Lemma('atlanta.n.01.Atlanta') Atlanta)\n","Hit!\n","\n","Word: 's\n","Tag: [\"'s\"]\n","\n","Word: recent\n","Tag: (Lemma('late.s.03.recent') recent)\n","\n","Word: ['primary']\n","Tag: (Lemma('primary.n.01.primary_election') primary election)\n","\n","Word: produced\n","Tag: (Lemma('produce.v.04.produce') produced)\n","\n","Word: ``\n","Tag: ['``']\n","\n","Word: no\n","Tag: ['no']\n","\n","Word: evidence\n","Tag: (Lemma('evidence.n.01.evidence') evidence)\n","Hit!\n","\n","Word: ''\n","Tag: [\"''\"]\n","\n","Word: that\n","Tag: ['that']\n","\n","Word: any\n","Tag: ['any']\n","\n","Word: irregularities\n","Tag: (Lemma('abnormality.n.04.irregularity') irregularities)\n","Hit!\n","\n","Word: ['took']\n","Tag: (Lemma('happen.v.01.take_place') took place)\n","\n","Word: .\n","Tag: ['.']\n","\n"]}],"source":["# for keeping track of which word and tag we're on\n","word_idx = 0\n","tag_idx = 0\n","\n","while tag_idx < len(tags[0]) and word_idx < len(sentences[0]):\n","    word = sentences[0][word_idx] #the current word\n","    tag = tags[0][tag_idx] #the tag for the current word\n","\n","    # check for tags that got assigned to compound words like primary_election\n","    if len(tag) > 1:\n","        print(\"Word:\",sentences[0][word_idx:(word_idx+len(tag)-1)])\n","        print(\"Tag:\",tag)\n","        word_idx += len(tag) #move to the next word that isn't part of the compound\n","\n","    # for Tree objects, check if it really tagged a word and not a group\n","    elif type(tag) is nltk.Tree and type(tag[0]) is str:\n","        print(\"Word:\",word)\n","        print(\"Tag:\",tag)\n","\n","        # here's how we can get the synset for tags that give us a Lemma\n","        if  type(tag.label()) != str:\n","            actual_sense = tag.label().synset()\n","            pred_sense = simplified_lesk(word,sentences[0])\n","            if actual_sense == pred_sense:\n","              print(\"Hit!\")\n","            #this is where you could check if you correctly matched the actual sense\n","\n","        word_idx += 1 #advance to next word\n","\n","    # check if it's a punctuation/stopword - if we got here, it means tag was not of type nltk.Tree\n","    elif type(tag[0]) is str:\n","        print(\"Word:\",word)\n","        print(\"Tag:\",tag)\n","        word_idx += 1\n","\n","    # If we get gerem it means the Tree contained a group of words, and we can count\n","    # how many with len( tag.leaves() )\n","    else:\n","        print(\"Word:\",word)\n","        print(\"Tag:\",tag)\n","        print(\"Words in this group:\",tag.leaves())\n","        word_idx += len(tag.leaves())\n","    tag_idx += 1\n","    print()"]},{"cell_type":"markdown","metadata":{"id":"RFgfBwXCmMzp"},"source":["## Applied Exploration\n","\n","For cases where the SemCor dataset has a single word tagged with a WordNet sense, run your `simplified_lesk` code on it and see if it matches. Go through all of the sentences in a particular file_id and compute an accuracy score.\n","\n","Write notes here on what you did and the results you got."]},{"cell_type":"code","source":[],"metadata":{"id":"keu2bD4T1pks"},"execution_count":null,"outputs":[]}],"metadata":{"celltoolbar":"Slideshow","colab":{"provenance":[{"file_id":"https://github.com/ericmanley/f23-CS195NLP/blob/main/F4_2_WordSenseDisambiguation.ipynb","timestamp":1697142855399}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"}},"nbformat":4,"nbformat_minor":0}